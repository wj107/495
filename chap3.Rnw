%--------495 report, Chapter 3------------


%---------------------------------------------------------------------------------
%--------------------- SECTION #3: Comparing GLM and NN
%---------------------------------------------------------------------------------

\section{A basic comparison of GLM and NN Predictive Models.}

Having received an introduction to two different types of predictive models, let's see an example of each in action.  We'll select two different 
datasets, consider three subsets of each in varying sizes (think small, medium, and large, here), and test predictive models for accuracy using 
50-fold cross validation on each data subset.  50-fold cross validation gives us a sampling of 50 standard errors from which we can assess the 
accuracy of each model.  As our two datasets will have different distributions for the response variables (quantitative versus categorical), we'll 
have several different questions for consideration: how do the standard errors differ for a GLM or a NN model?  Is this the case for both a 
quantitative response variable and a categorical response?  Does more data mean more accuracy?  Or does more data mean more time needed to build a 
model?  How much more time?

We'll look at the datasets we are considering, we'll look at the R code used to build and test the models, and then we'll summarize and analyze the 
results.

%-------------------------------------------------------------------------
%----------------Section 3.1: DATASETS.
%-------------------------------------------------------------------------

\subsection{The Datasets.}
	For this paper, I will train both GLM and NN models using two pre-selected datasets.  The datasets that will be considered are a listing of 
various attributes for nearly 54 000 diamonds, and team statistics for nearly 3000 
basketball games from the NBA.  The diamonds data is provided (without source) as part of the \texttt{ggplot2} package for R, and it includes 10 
variables for each of the diamonds.

	<<chunk31, echo=F, size="footnotesize", comment=NA, background="grey80">>=
	head(diamonds)
	@

The gamelog dataset was retrieved from the website \href{https://www.basketball-reference.com}{Basketball-Reference.com}, and it looks at 21 
statistics for each basketball game.  

	<<chunk32, echo=F, size="scriptsize", comment=NA, background="grey80">>=
	load("R/gamelog.RData")
	head(datt)
	@

When creating our predictive models for analysis, we will consider the quantitative variable \texttt{price} as our response variable for the 
diamonds dataset, and the categorical variable \texttt{WL} (a value of texttt{1} indicates the team won a game, \texttt{0} denotes a loss) as our 
response variable for the gamelog dataset.  All other variables will be considered as predictor variables; thus our formulas will be 
\texttt{price $\sim$ .} and \texttt{WL $\sim$ .} when we generate our predictive models.

The distributions of the response variables for these datasets is illustrated in Figure \#\ref{responsedistribs}.  With the different distributions
for these two datasets, we can analyze the performance of GLM and NN predictive models when given either a quantiative response variable or a 
categorical response variable.

	%responsedistribs
	\begin{figure}[h!]
	\includegraphics[height=1.5in]{{graph1.png}}
	\includegraphics[height=1.5in]{{graph2.png}}
	\caption{Distributions for the response variables in \texttt{diamonds} dataset (left), and the \texttt{gamelog} dataset (right)}
	\label{responsedistribs}
	\end{figure}

One aspect for our analysis of predictive models involves varying the size of the training sets for building the predictive models.  Therefore,
rather than consider the entire \texttt{diamonds} or \texttt{gamelog} dataset as a whole, we will draw three different sized subsets from each to
consider for analysis -- allowing us to consider if more data leads to more accurate predictions from our model.  Consider the following code to
draw three subsets from the \texttt{diamonds} dataset:

	<<chunk33, eval=F, size="footnotesize", comment=NA, background="grey80">>=
	diamonds.small<-diamonds[sample(nrow(diamonds),550),]
	diamonds.medium<-diamonds[sample(nrow(diamonds),1100),]
	diamonds.large<-diamonds[sample(nrow(diamonds),2200),]
	@

This code is creating three subsets from the entire \texttt{diamonds} dataset.  The small subset contains a random sample of 550 datapoints (that 
is, the information on 550 different diamonds), the medium subset is a random sample of 1100 diamonds, and the large subset a sample of 2200 
diamonds.  We will define the subsets \texttt{gamelog.small}, \texttt{gamelog.medium}, and \texttt{gamelog.large} in an analogous manner.

When we carry out the process of cross-validation on these data subsets, we will use a 10-to-1 train-test ratio (c.f. the benchmark 70-30 train-test
ratio) that allows us to build our predictive models with training sets of sizes 500, 1000, and 2000, and testing sets of sizes 50, 100, and 200.  
For details on model building and testing\dots

%----------------------------------------------------------------------
%----------------SECTION 3.2: se.estimate 
%----------------------------------------------------------------------

\subsection{\texttt{se.estimate} function.}

In order to analyze the standard errors for our predictive models, we will use the process of cross-validation.  As mentioned previously, that 
requires (1) dividing our data into \textit{train} and \textit{test} subsets, (2) building a predictive model from the training set, (3) testing
the model on the testing set, and calculating the standard error, and (5) iterating this process.  That's an enormous amount of calculation; and
using R to script the process will make things far easier.

I wrote an R script that creates a function, \texttt{se.estimate}, to accomplish these tasks and record the results for analysis.  The arguments of
the function (along with the default values) are given below.

	<<chunk34, echo=F, background="grey80", comment=NA, size="footnotesize">>=
	source("../SEestimate/SEestimate.R")
	head(se.estimate,3)
	@
	
(Note that, in a remarkable instance of mislabelling, the argument \texttt{pred.var} refers to the \textit{response} variable for the data.  
Whoops!) Using the \texttt{se.estimate} function, we are able to easily perform cross-validation on our datasets, collecting samplings of the 
standard errors for analysis.  For instance, the function call:

	<<chunk35, echo=T, eval=F, background="grey80", size="footnotesize", comment=NA>>=
	se.estimate(dat=cdc, pred.var=weight, resamples=10, test.pct=0.30, 
	method="glm", timer=T)
	@

Would perform 10-fold cross-validation on the dataset from the CDC survey, using a 70/30 train/test split and using GLM predictive models, while
recording the time needed to build each GLM model.  The output from this function call would be list of length three, illustrated below.

	<<chunk36, echo=T, eval=F, background="grey80", size="footnotesize", comment=NA>>=
	se.estimate(dat=cdc, pred.var=weight, resamples=10, test.pct=0.30, 
	method="glm", timer=T)
	@

The first entry of the output gives a vector (in this case of length 10) containing the standard errors from each resampling.  The second entry of 
the output is a five-number summary of the standard errors.  The third entry is the \textit{build time}, given in seconds, that is needed for the 
computer to construct each of the GLM models.  The full code for the \texttt{se.estimate} function is given in the bibliography [cite github].

It is from repeated calls of the \texttt{se.estimate} function, for the small, medium, and large sized subsets of the \texttt{diamonds} and 
\texttt{gamelog} datasets, that we will collect information on standard errors and build time for our analysis.  Two of these function calls are 
given below.  (Note that \texttt{method="both"} indicates that the \texttt{se.estimate} function will build both GLM and NN
predictive models for the selected data subset.)

	<<chunk37, eval=F, background="grey80", size="footnotesize", comment=NA>>=
	diamonds.small.result<-se.estimate(dat=diamonds.small, pred.var=price, 
		resamples=50, test.size=50, method="both", distrib="continuous",
		timer=T)
	gamelog.small.result<-se.estimate(dat=gamelog.small, pred.var=WL, 
		resamples=50, test.size=50, method="both", distrib="categorical",
		timer=T)
	@

It is easy to see how these two function calls can be generalized to the medium and large data subsets we are considering, producing for us output
data to compare and analyze\dots 

%----------------------------------------------------------------------
%----------------SECTION 3.3: Results for DIAMONDS 
%----------------------------------------------------------------------

\subsection{Results for predictive models built from \texttt{diamonds}.}
	After calling the \texttt{se.estimate} function for the subsets of the \texttt{diamonds} data, we have a sampling of 300 standard errors, 
after the script built and tested 300 predictive models. In Figure \#\ref{diamondsgraph}, we organize this sampling according to the type of predictive model 
(GLM or NN) and the size of the testing subset for the model (50, 100, or 200).

	%diamondsgraph
	\begin{figure}[h!]
	\centering
		\includegraphics[height=2in]{{diamondsgraph.png}}
	\caption{Graphical summary of standard errors built from \texttt{diamonds} dataset}
	\label{diamondsgraph}
	\end{figure}

Looking at the graph, we notice that the general clustering of the standard error data points \textit{decreases} as the size of the
testing set increases; so the reasonable hunch that `more data means more accuracy' seems to be supported, in this case.  Also, the blue
dots, the standard errors for the neural net models, are noticeable below the red (GLM) dots.  That is, the neural net predictive models, 
on the whole, were more accurate than the GLM models.  The results, summarized in tabular form, paint this same picture:

	<<chunk38, echo=F, background="grey80", size="footnotesize", comment=NA>>=
	require(knitr)
	load("R/results2.RData")
	tapply(results2[,4],results2[,2:3],mean)->table1
	kable(table1)
	@

Another observation we might expect after repeating the cross-validation with larger training sets is for the variation in the standard errors 
to decrease.  That is, we might expect to narrow in on the `true' standard error when the training sets were larger.  This would be similar to the 
Law of Large Numbers, but for data sampling -- more data, more representative samplings, more tendency to cluster around an (ostensibly) `true' 
value for the standard error.  Although we can't conclusively test this property from the data in Figure \#\ref{diamondsgraph} above, when looking 
at a numerical table we can check:

	<<chunk39, echo=F, background="grey80", size="footnotesize", comment=NA>>=
	tapply(results2[,4],results2[,2:3],sd)->table2
	kable(table2)
	@

Surprisingly, the accuracy only improves for the neural net model -- for the GLM predictive models, the variation does not uniformly improve.  That
means, although in general more data makes the models more accurate, with GLM more data can still give rise to a `bad' model that is inconsistent 
with the general performance of other similar models.

It's one comparison, from one data set.  But the numbers seem to clearly indicate:  more data is more accuracy, and neural nets are more accurate 
and more consistent!  Are we ready to write this in stone?  Well, let's check another data set for corroborating evidence, first\dots

%----------------------------------------------------------------------
%----------------SECTION 3.4: Results for DIAMONDS 
%----------------------------------------------------------------------

\subsection{Results for predictive models built from \texttt{gamelog}.}
After calling the \texttt{se.estimate} function for the subsets of the \texttt{gamelog} data, we have a sampling of 300 standard errors, 
after the script built and tested 300 predictive models. In Figure \#\ref{gameloggraph}, we organize this sampling according to the type of 
predictive model (GLM or NN) and the size of the testing subset for the model (50, 100, or 200).

	%gameloggraph
	\begin{figure}[h!]
	\centering
		\includegraphics[height=2in]{{gameloggraph.png}}
	\caption{Graphical summary of standard errors built from \texttt{gamelog} dataset}
	\label{gameloggraph}
	\end{figure}

Surprisingly, the first observation is that neural net predictive models are notably \textbf{less} accurate in this example.  Across all three
sample sizes, the GLM models showed better accuracy.  The results, summarized in tabular form:

	<<chunk310, echo=F, background="grey80", size="footnotesize", comment=NA>>=
	load("R/results3.RData")
	tapply(results3[,4],results3[,2:3],mean)->table1
	kable(table1)
	@

Notice too that the improvement in the gains in accuracy as the training sets grow are negligible.  Examining standard error:

	<<chunk311, echo=F, background="grey80", size="footnotesize", comment=NA>>=
	tapply(results3[,4],results3[,2:3],sd)->table2
	kable(table2)
	@

Surprisingly, the accuracy only improves for the neural net model -- for the GLM predictive models, the variation does not uniformly improve.  That
means, although in general more data makes the models more accurate, with GLM more data can still give rise to a `bad' model that is inconsistent 
with the general performance of other similar models.

It's one comparison, from one data set.  But the numbers seem to clearly indicate:  more data is more accuracy, and neural nets are more accurate 
and more consistent!  Are we ready to write this in stone?  Well, let's check another data set for corroborating evidence, first\dots
