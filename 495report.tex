%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%% 495 Report: comparing standard error between GLM and NN models
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------
%-------- preamble
%----------------------------------

%---set font size, options for document
	\documentclass[12pt]{article}  
%---set numbering depth
	\setcounter{secnumdepth}{2}
%---get packages!
	%---for math notations
	\usepackage{amsmath}
	%---for including graphics	
	\usepackage{graphicx}
	%---for hyperlinks
	\usepackage{hyperref}
	%---hyperlink format
	\hypersetup{colorlinks=true,urlcolor=blue}

%---set graphicspath
	\graphicspath{
		{img/}
		}
	
%%%%%%%%%%%%%%%% DOCUMENT ENVIRONMENT %%%%%%%%%%%%%%%%%%%
\begin{document}

%---top matter
	\title{Generalized Linear Models and Neural Nets: Comparison Between Methods for Predictive Analysis}
	\author{William R. Johnson}
	%%%%other title commands??  advisor?  
	\date{12 February 2018}
	\maketitle

%---abstract
	\renewcommand{\abstractname}{Abstract:}
	\begin{abstract}
		Whether a \href{https://fivethirtyeight.com/features/oscars-2018-early-predictions-nominations/}{fluffy article about primetime 
awards programs on the website fivethirtyeight.com}, or deep within the analytical 
department of a multi-billion dollar, multinational corporations, predictive analysis is a critical part of our world in the age of ``Big Data".  
In this paper, I will examine two types of predicitive analysis, Generalized Linear Models (GLM) and Neural Nets (NN).  I will offer a cursory 
overview of the mathematics behind each type of model, and using the R statistical software, I will analyze the accuracy of both GLM and NN 
predictive models, as well as the time needed to construct each model. I will consider an approximation of the standard error as a measure of model 
accuracy, and I will compute these approximations using 50-fold cross-validation.  The models will be fit to two different datasets -- one with a 
normally distributed response variable, and one with a binomially distributed response.  The results -- estimated standard errors and processing 
time -- will be analyzed across the different model types, the different response variables, and different sizes of training sets.
	\end{abstract}
	\vfill\eject


%------------------------------
%-------------body
%------------------------------

%---------------------------------------------------------------------------------------------------------------
%---------------SECTION #1: INTRO, citation of GLM/NN in scholarly works, overview of my study------------------
%---------------------------------------------------------------------------------------------------------------

	\section{Intro to Predictive Analysis.}
		We live in the age of `Big Data'.  Using that data to measure, observe, and predict has great value:  economically, culturally, 
socially.  It's easy to say that this prediction is done by an app -- highlight data, point, click, ta-da!  You can find \href{https://www.google.com/maps/dir/Chicago,+Illinois/Champagne+Beach,+Vanuatu/@10.9584518,-169.5679877,3z/data=!4m13!4m12!1m5!1m1!1s0x880e2c3cd0f4cbed:0xafe0a6ad09c0c000!2m2!1d-87.6297982!2d41.8781136!1m5!1m1!1s0x6ef24c13ce5a7303:0x5807cbc94589a4bb!2m2!1d167.1203814!2d-15.1437919}{estimated travel time}, 
\href{https://www.amazon.com/Statistics-Dummies-Math-Science/dp/1119293529/ref=sr_1_1?ie=UTF8&qid=1518456852&sr=8-1&keywords=statistics+for+dummies+2017}{`Related materials' in consumer recommendations}, and \href{http://www.espn.com/mens-college-basketball/game?gameId=400986579}{predicted outcome of sporting events} with a few simple taps on your 
phone.  But what's the mathematical machinery behind these predictions?  Are all the predictions calculated in a similar manner?  How accurate is a
prediction?  How long would it take to compute a prediction, for a given set of data?  All of these questions are critical to ensure that predictive models 
remain practical and functional in our `Big Data' world.

Now, I will not discuss the specific predictive methods for proprietary applications, like the aforementioned travel predictions from Google or 
suggested product recommendations from Amazon.  But I will offer a cursory examination of two major mathematical methods for predictive analysis:
Generalized Linear Models (GLM), and Neural Networks (NN).  Both methods allow us to `train' mathematical models on an existing data set, which can
subsequently be used to predict future outcomes, or \textbf{response variables}.  Figure \#1 contains two graphs that illustrate, graphically, these 
two types of mathematical models for a simple data set.

Given a sample of heights and weights from 20000 individuals in the United States collected from a 2015 survey conducted by the Centers for Disease
Control, the red lines indicate the predicted response variables from mathematical models trained on this data.  

\begin{figure}[h!]
\centering
\includegraphics[height=1.8in]{{model1.png}}
\includegraphics[height=1.8in]{{model2.png}}
\caption{Predictive models for height and weight data, trained using GLM methods (left), and NN methods (right)}
\end{figure}

These are simple examples; the data set has only two variables, and intuitively most people have an understanding of how these variables relate 
without appealing to mathematics.  But the underlying principle -- that the data can `train' a predictive mathematical model -- is one that we will
observe and analze in many other contexts over the course of this paper.

%---------------SUBSECTION #1.1: GLM in scholarly works---------------------

	\subsection{GLM in Mathematical Literature.}
		For many scientific breakthroughs, there is no one, clear, defining `AHA!' moment where a new idea was discovered from scratch. For
GLM, however, the is a moment that approximates that: the publication in 1972, by Nelder and Wedderburn, of the paper `Generalized Linear Models'
in the \textit{Journal of the Royal Statistical Society}.  This often-cited work clearly laid out many of the ideas that are central to the study 
of GLMs today.  On the back of this work, and with the rise of widely-available statistical software to calculate GLM models such as R and SPSS, 
these models have been widely applied across the scientific literature in service of predictive analysis.  For instance, GLM has utility in
analyzing systems as varied as 
\href{https://www.sciencedirect.com/science/article/pii/S037877961630222X}{hydropower generation}, 
\href{https://www.sciencedirect.com/science/article/pii/S1470160X17306271}{habitat selection for small mammals}, and 
\href{https://www.sciencedirect.com/science/article/pii/S0167668715303358}{insurance claims}.  

Certainly, if GLM is used to obtain predictions of economically or ecologically important data, as suggested in the links above, we would want to 
ensure that the predictions are accurate. We will demonstrate in this paper how to estimate the standard error for a GLM model, which can be 
used to draw up confidence intervals and quantify the accuracy of a prediction.  Having these estimates for accuracy can be vital in determining
whether or not a predictive model is a valid basis for making policy decisions that carry great economic or environmental risk.

%---------------SUBSECTION #1.2: NN in scholarly works----------------------

	\subsection{NN in Mathematical Literature.}
		From the earliest work on \textit{threshold logic} by McCulloch and Pitts in 1943, to the explosion of machine learning algorithms
in use today in the age of `Big Data', the idea that the architecture of computing systems could be inspired by the architecture of the human brain
and its neural networks has maintained a presence in the scientific literature.  With the dramatic rise in computational power in the last 
generation along with statistical software such as R and SPSS, interest in neural networks has flourished, along with its place in scientific
research.  We can see examples of current research using neural nets to analyze topics as diverse as 
\href{https://www.sciencedirect.com/science/article/pii/S1877050918300656}{the gender of Russian authors},
\href{https://www.sciencedirect.com/science/article/pii/S2468203916300024}{the surface radiation in the Sundarban forest in India}, and 
\href{https://www.sciencedirect.com/science/article/pii/S1877050917303617}{routes through the city of Zhengzhou chosen by tourists}.
		
A distinct characteristic of neural nets is that of \textit{machine learning} -- given more data to `train' the model, it can improve the accuracy
of the response variable.  However, adding more data to a neural network requires more computational muscle, and for large or complex datasets,
it can require significantly more time to process.  So, similar to GLM, it will help us to have a means to check how accurate our predictions are,
so we can determine whether or not the added data and accuracy is worth the extra time needed for processing.  As with GLM, our means of assessing
accuracy in this paper will be to approximate the standard error for a given NN model.

%---------------SUBSECTION #1.3: Predictive models and this paper-----------


	\subsection{Methodolgy.}

	$$professor's suggestion:  move \S 1.3 to chapter 3.$$
$$My suggestion:  replace this with \S1.3 on accuracy of predictive models in literature??$$

		For this paper, I will train both GLM and NN models using two pre-selected datasets.  As mentioned above, for each type of
predictive model, we can approximate the standard error for each model, quantifying for us the accuracy of each model.  We can then compare the 
standard errors between the two models, hopefully drawing some conclusions about the relative accuracy of one model compared to the other.

The datasets that will be considered are a listing of various attributes for nearly 54 000 diamonds, and team statistics for nearly 3000 
basketball games from the NBA.  The diamonds data is provided without source as part of the `ggplot2' package for the R statistical software, while
the gamelog dataset was retrieved from the website \href{https://www.basketball-reference.com}{Basketball-Reference.com}.  The diamonds dataset includes 
10 variables for each of the diamonds, while the gamelog dataset looks at 21 statistics for each basketball game.  When creating predictive models, we will 
consider the quantitative variable \textit{price} as our response variable for the diamonds dataset, and the categorical variable \textit{WL} (whether a team won 
or lost a basketball game) as our response variable for the gamelog dataset.  A motivation for working with these two datasets is the fact that the  
response variables for each dataset are distributed differently, as illustrated in the graphs in Figure \#2.

\begin{figure}[h!]
\includegraphics[height=1.5in]{{graph1.png}}
\includegraphics[height=1.5in]{{graph2.png}}
\caption{Distributions for the response variables in DIAMONDS dataset (left), and the GAMELOG dataset (right)}
\end{figure}

We can easily train calculate a GLM or NN model using an entire dataset, and then compare the predicted results from these models against the
actual response variables.  But the weakness of this method is that we are not introducing any new data for testing accuracy.  Instead, dividing 
the data into disjoint `train' and `test' subsets gives us a better idea of how accurate a predictive model would be given new, independent data.
Carrying out this `train-test' process once is prone to great variability, depending on the particular choice of dataset selected to train the 
model.  Instead, repeating this `train-test' process a number of times across repeated random samples of the dataset will give a better
picture of the accuracy for a predictive model.  This repeated resampling and model testing is known as \textit{cross validation}, and we will 
carry it out 50 times (50-fold cross validation) for both GLM and NN predictive models with both of our datasets.

In addition, the size of the `train' subset would likely affect the accuracy of a given model.  With a larger training set, we would anticipate
that a predictive model would be more accurate.  However, as alluded to above, the larger training set also will cost us in terms of processing
time.  To examine how the size of the training set affects model accuracy (and processing time), we will run each instance of 50-fold cross
validation three times, with training sets of size 500, 1000, and 2000.  

To measure accuracy for each of the models, we will calculate the observed standard error of the test subset.  The computations will be carried out
using the open-source R statistical software.

%------------------------------------------------------------------------------------------
%----------- SECTION #2: THE PARTS OF THE PREDICTIVE MODELS
%------------------------------------------------------------------------------------------

	\section{The Parts of the Predictive Models.}
In the first section of this paper, I presented two side-by-side graphs, one representing a GLM predictive model, and the other representing a NN
predictive model.  But I presented the two graphs without any description of how they were created, other than alluding to the use of statistical
computer software to carry out the computations.  The computer certainly makes the process of model creation easier; but I can't say ``Hey, R, make
me a GLM model, please!"  (Although it's not too far off, frankly\dots)  Even if the computer is doing the heavy lifting, I need to understand the
fundamental parts of a GLM model, and I need to understand some critical attributes of my data before a workable predictive model is created.  In 
this section, without wading too deeply into theoretical matters, we'll look at the parts of a predictive model, so we're able to understand the 
information we're responsible for feeding into the computer before it does its 1's and 0's thing and returns us a predictive model.

%-------------------SUBSECTION #2.1:  GENERALIZED LINEAR MODELS-----------------

	\subsection{The Parts of GLM.}
	
	\subsubsection{Linear Models.}

To understand a GLM predictive model, examining a very basic linear regression model first will be instructive.  In Figure \#3, we can see four
critical parts of a predictive model:  the observed data (the red dots in the graph), the predictor variable (tonnage of lemon imports), the 
response variable (highway fatality rate), and a linear relationship established between the two variables (an equation of the form 
$Y = \beta_0 + \beta_1 X$).  If our data set has multiple predictor variables, we can consider a multiple regression equation 
$Y = \beta_0 + \hat{\beta_1} X$ where $X$ is a column matrix and $\hat{\beta_1}$ is a row matrix, with entries for each of the predictor variables.  
Although it's more challenging to visualize in a multiple regression, the underlying goal is the same as what is shown in Figure \#3: finding 
coefficients $\beta_i$ so that the resulting linear regression equation `fits' the data most closely.  The process of finding `best fit' 
coefficients is easily carried out by statistical software -- but we need to be able to specify the observed data, the predictor and response 
variables, in order to get the coefficients $\beta_i$ for a linear model.

\begin{figure}[h!]
\includegraphics[height=1.5in]{{regression1.jpg}}
\caption{A basic linear regression with dataset, predictor and response variables, and linear model.}
\end{figure}

	\subsubsection{Standard Error.}
In Figure \#3, we can see that the linear model -- the predictive model -- differs from the data points slightly.  That difference is referred to
as the \textit{residual}, and the goal of a regression model is not to minimize the sum of the residuals, but to minimize the sum of the 
\textit{squares} of the residuals.  This sum, $\sum (\bar{Y} - Y)^2$, is referred to as the \textit{standard error}.  

Standard error in model creation, standard error in testing model for accuracy.

	\subsubsection{Generalized Linear Models.}
	Consider a dataset.... pts scored, W/L.

	\subsubsection{The Parts of a Generalized Linear Model.}
	In conclusion, dataset, vairables, response, link function, test data set, standard error calculation	




%--------------for now!!
\end{document}




\section{Neural Nets.}

\subsection{An Introduction to Neural Nets.}
To offer a succinct definition of a neural net and how it works would be as simple as explaining how you know a chair when you see 
one.  That is, your experience with all different forms of chairs, all the different styles of chairs, and all the different ways someone might use
a chair allows you to (fairly definitely) state if something is a chair or not.  To use language more similar to those referencing mathematical
models, your lifelong history experiencing chairs is a dataset that has `trained' you to know whether or not a given object is a chair.  In a 
similar vein, if a neural net is `trained' on a given dataset, it can output information about a pre-determined response variable.



\subsection{A basic NN example.}
We can see Neural Nets in action if we consider the data set....  As before, we will use R to construct a Neural Net, from the functions available
in the -- surprise! -- `neuralnet' package.

\section{A Comparison of GLM and NN.}
Having looked at both GLM and Neural Nets individually, now we can examine more closely how the two methods stack up when they aim to create a 
predictive model for the same data set.  Which model offers more accurate predictions?  What data set offers more consistent predictions?  Is there
an appreciable difference between the two methods?  To weigh in on this comparison, I will (1) demonstrate how to divide a data set into `train' 
and `test' subsets so we can create and test predictive models (2) outline R code that calculates predictive models for both methods, (3) compare 
the responses from the two predictive models, both numerically and graphically, and (4) draw some observations from the comparisons.

\subsection{A Data Set for our Predictions.}
In the `real world' of big data predictions, there is collected data and future data.  The collected data is used to create a predictive model, and
the hope is it can be tested (and refined) in the future as more, newer data becomes available.  Considering present/future data is not feasible 
for this paper; I'm not going to create a predictive model here with current and come back to write Volume \#2 in six months after testing future 
data.  We need to `simulate' current and future data from existing, collected data.

To accomplish this, the R package `caTools' offers the function `sample.split'.  This function allows us to ....

\subsection{Creating Predictive Models in R.}
With our `train' and `test' data sets in hand, it's time to create predictive models.  In previous sections, we saw how to accomplish this in R.  
However, since our focus is on offering a general comparison for ANY multi-variable data set, we need to fine-tune the code a bit.  To that end...

\subsection{A Comparison of the Predictive Models.}
The above code allowed us to see, side-by-side, the predicted and actual responses for our data set, and the standard errors for the predictions.
We can analyze these outputs both numerically and graphically.  For instance....

\subsection{Observations from the Comparison.}
After looking at the predictions and responses from the two methods, we can see that....


\section{Topics for Further Study.}
Having completed a very basic side-by-side comparison of GLM and Neural Net predictive methods for data sets, we can begin to focus on some 
questions that might offer opportunities for further exploration.  For instance....



%%%bibliography, etc

\end{document}
