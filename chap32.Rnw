%-----------495 report, Chapter 3---------------------
  
%----------------------------------------------------------------------
%----------------SECTION 3.2: se.estimate 
%----------------------------------------------------------------------
  
  \subsection{\texttt{se.estimate} function.}

In order to analyze the standard errors for our predictive models, we will use the process of cross-validation.  As mentioned previously, that 
requires (1) dividing our data into \textit{train} and \textit{test} subsets, (2) building a predictive model from the training set, (3) testing
the model on the testing set, and calculating the standard error, and (5) iterating this process.  That's an enormous amount of calculation; and
using R to script the process will make things far easier.

I wrote an R script that creates a function, \href{https://github.com/wj107/SEestimate}{\texttt{se.estimate}} \cite{johnson}, to accomplish these tasks and record the results for analysis.  The arguments of
the function (along with the default values) are given below.

<<chunk34, echo=F, background="grey80", comment=NA, size="footnotesize">>=
source("../SEestimate/SEestimate.R")
head(se.estimate,3)
@

(Note that, in a remarkable instance of mislabelling, the argument \texttt{pred.var} refers to the \textit{response} variable for the data.  
Whoops!) Using the \texttt{se.estimate} function, we are able to easily perform cross-validation on our datasets, collecting samplings of the 
standard errors for analysis.  For instance, the function call:

<<chunk35, echo=T, eval=F, background="grey80", size="footnotesize", comment=NA>>=
se.estimate(dat=cdc, pred.var="weight", resamples=10, test.pct=0.30, 
method="glm", timer=T)
@

\noindent would perform 10-fold cross-validation on the dataset from the CDC survey, using a 70/30 train/test split and using GLM predictive models, while
recording the time needed to build each GLM model.  The output from this function call would be list of length three, illustrated below.

<<chunk36, echo=F, background="grey80", size="footnotesize", comment=NA>>=
load("R/cdc.RData")
se.estimate(dat=cdc, pred.var="weight", resamples=10, test.pct=0.30, method="glm", timer=T,prog.bar=F)
@

The first entry of the output gives a vector (in this case of length 10) containing the standard errors from each resampling.  The second entry of 
the output is a five-number summary of the standard errors.  The third entry is the \textit{build time}, given in seconds, that is needed for the 
computer to construct each of the GLM models.  The full code for the \texttt{se.estimate} function is given in the bibliography [cite github].

It is from repeated calls of the \texttt{se.estimate} function, for the small, medium, and large sized subsets of the \texttt{diamonds} and 
\texttt{gamelog} datasets, that we will collect information on standard errors and build time for our analysis.  Two of these function calls are 
given below.  (Note that \texttt{method="both"} indicates that the \texttt{se.estimate} function will build both GLM and NN
predictive models for the selected data subset.)

<<chunk37, eval=F, background="grey80", size="footnotesize", comment=NA>>=
diamonds.small.result<-se.estimate(dat=diamonds.small, pred.var=price, 
resamples=50, test.size=50, method="both", distrib="continuous",
timer=T)
gamelog.small.result<-se.estimate(dat=gamelog.small, pred.var=WL, 
resamples=50, test.size=50, method="both", distrib="categorical",
timer=T)
@

It is easy to see how these two function calls can be generalized to the medium and large data subsets we are considering, producing for us output
data to compare and analyze\dots 
