%-------------495 report, Chapter 2-----------


%------------------------------------------------------------------------------------------
%----------- SECTION #2: THE PARTS OF THE PREDICTIVE MODELS
%------------------------------------------------------------------------------------------

	\section{The Parts of the Predictive Models.}
In the first section of this paper, I presented two side-by-side graphs, one representing a GLM predictive model, 
and the other representing a NN predictive model.  But I presented the two graphs without any description of how they were created, other than 
alluding to the use of statistical software to carry out the computations.  The computer certainly makes the process of model creation easier; but 
I can't say ``Hey, R, make me a GLM model, please!"  (Although it's not too far off, frankly\dots)  While the computer is doing the heavy lifting, 
I still need to understand the fundamental parts of a GLM model, and I need to understand some critical attributes of my data before a workable 
predictive model is created.  In this section, without wading too deeply into theoretical matters, we'll look at the parts of a predictive model, 
so we're able to understand the information we're responsible for feeding into the computer before it does its 1's and 0's thing and returns us a 
predictive model.  First, we'll break down general concepts that we'll find in any predictive model, and in the second and third sections we'll 
consider details specific to GLM and NN models.

%-------------------SUBSECTION #2.1:  PARTS of a PREDICTIVE MODEL-----------------
\subsection{The General Parts of a Predictive Model.}
	In the first chapter, in Figure \#\ref{intromodels}, we saw graphical representations of two different predictive models:  one created using GLM, one from 
NN.  Though the two models have clear points of distinction, we can also identify some similarities.  These similarities represent some of the 
fundamental parts in any predictive model.  Here, we aim to introduce precise terminology that we can use to describe these similarities, and that 
can help us outline the goals of our predictive model analysis for this paper in a clear, unambiguous manner.

	%---------------OBSERVED DATA-------------
	\subsubsection{Observed Data.}
	The first thing, before you can even consider a predictive model:  we need to have data to draw a prediction from.  No data, no model.  And,
of course, we want our model grounded in reality -- so it's important to verify that your data consists of well sourced observations before 
beginning to construct any model.  There's a well-known phrase among programmers:  `Garbage in, garbage out.'  Your predictive model is only as 
good as your data.  Make sure the process of model building starts atop a legitimate source of \textit{observed data}.  For example, the observed 
that was used to create the predictive models in the previous section was derived from the 
\href{https://www.cdc.gov/brfss/index.html}{Behavioral Risk Factor Surveillance System (BRFSS)}, a system of nation-wide health surveys.  These 
surveys have a well-documented methodology, they are conducted annually, and they are administered by the Centers for Disease Control.  The header 
for this dataset, which lists the names of the nine observed variables, is given in Figure \#\ref{cdchead}.

	%cdchead
	\begin{figure}
	\centering
		<<c21, echo=F, size="footnotesize", comment=NA, background="grey80">>=
		load("R/cdc.RData")
		head(cdc)
		@
	\caption{Header for the \texttt{cdc} dataset}
	\label{cdchead}
	\end{figure}


	%---------------VARIABLES----------
	\subsubsection{Variables.}
	With observed data to work from, it is necessary to identify and classify the variables within the data set.  In particular, 
we need to classify the variables for data in one of three different ways: as a \textbf{predictor variable}, a \textbf{response variable}, or a 
variable to ignore in our model.  

In Figure \#\ref{cdchead} we can easily identify the nine different variables (and determine if they are quantitative or categorical).  In Figure
\#\ref{intromodels}, we designated \texttt{height} as the predictive variable, and \texttt{weight} as the response variable.  The other seven 
variables were ignored in our model.

	Is it necessary to ignore all those othe seven variables? Not at all -- if we have a particular response variable of interest to us, we can select a number
of predictor variables that we might suspect are related.  For example, we could easily consider \texttt{height} and \texttt{age} as predictor variables in the previous example.  We could designate \textbf{every} variable (other than \texttt{weight}) as a predictor.  More predictors mean our models can quickly become complex (and thus, we lose the nice visualizations from Figure \#\ref{intromodels}) and they might take longer to build, but they have the potential
to exhibit greater accuracy.  The decision of what variables to include and exclude from a model involves a trade-off between model accuracy and simplicity 
-- and could easily be the basis for further analysis, though these questions are beyond the scope of this paper.

	%---------------FORMULAS-----------
	\subsubsection{Formulas.}
	After identifying the predictor and response variables, we can also write a formula that describes this relationship between the variables.
If we were to consider \texttt{height} and \texttt{age} as predictor variables for the CDC data, we would write \texttt{weight $\sim$ height + age},
with the left-hand argument signifying our response variable, and the right-hand sum comprised of the predictor variables.  (Could we add exponents
or other operations to these terms?  Sure\dots but that's for another paper.)  Read aloud, we might say ``Weight is related to height and age". 
\textit{Is related to} is fairly vague language, but as we'll see later on, the nature of the relationship depends on a number 
of other factors -- so we cannot offer a stronger characterization, here.

	If we were to consider \texttt{weight} as the response variable, and all eight other variables as predictor variables, we would have the 
following formula:

	<<chunk22, echo=F, size="scriptsize", comment=NA, background="grey80">>=
	f<-"weight ~ genhlth + exerany + hlthplan + smoke100 + height + wtdesire + age + gender"
	f
	@

For a situation like this, where we do not ignore any variables, we can abbreviate it as \texttt{weight $\sim$ .} with the dot indicating that all 
remaining variables are predictors.

	%--------------CROSS VALIDATION------------
	\subsubsection{Cross Validation.}
With our observed data set, and a formula that outlines the relationship we posit in our data, we could begin to create a predictive model using our entire 
dataset.  Furthermore, we could take this predictive model and test it: we could compare the model's predicted response values for against the actual response 
variables in the data.  The weakness of this method is that we are not introducing any new data for testing accuracy.  

A solution to this issue is to divide the data into disjoint subsets, called the \textbf{training set} and \textbf{testing set}.  From here, we 
create the model using the training set, and we test the model for accuracy using the testing set.  Separating out this process gives us a better 
idea of how accurate a predictive model would be if it was given new, independent data.  However, carrying out this `train-test' splitting of the data just 
once is prone to great variability, depending on the particular choice of dataset selected to train the model.  Instead, repeating this `train-test' 
splitting a number of times, repeatedly drawing random samples from the data as training and testing sets, will give a better picture of the 
accuracy for a predictive model.  This repeated resampling and model testing is known as \textit{cross validation}.  In Figure \#\ref{crossvalidation},
a visual representation of the process is given, where the train-test split is performed four times: 4-fold cross validation.

	%crossvalidation
	\begin{figure}[h!]
	\centering
		\includegraphics[height=15em]{{crossvalidation.jpg}}
	\caption{Visual representation of cross-validation.  Created by Fabian Fl\"{o}ck.}
	\label{crossvalidation}
	\end{figure}


The ratio of the sizes of the train and test subsets can be adjusted, though a larger training subset will likely exhibit less variability across
repeated resamplings.  For this reason, a common ratio of the size of training and testing sets is 70-30.  Note that, in Figure \#\ref{crossvalidation},
the example uses a 75-25 split, or 3-to-1 train-test ratio.




	%--------------RESIDUALS and STANDARD ERROR--------
	\subsubsection{Residuals and Standard Error.}
Testing the accuracy of a predictive model is of critical, given the numerous ways in which these models are applied to real-world 
applications of great economic, social, or environment importance.  A common test for accuracy with a predictive model is the standard error, as 
\href{https://www.sciencedirect.com/science/article/pii/S0165176518300855}{a number} of 
\href{https://www.sciencedirect.com/science/article/pii/S030441651730404X}{scientific research articles} have 
\href{https://www.sciencedirect.com/science/article/pii/S0165176516303718}{analyzed how to estimate this quantity}.

	Above we alluded to how we can consider accuracy for a predictive model: we can compare the model's predicted values for the response
variable against the actual response variables in the data.  The difference between these values, the predicted and the actual response, is called
the \textbf{residual}.  For example, in Figure \#\ref{residual}, the NN predictive model for the height and weight of 20000 people is given, as it
was in Figure \#\ref{intromodels}.  Using myself as the 20,001st data point, I plotted my height and weight (5'6", 144 pounds) with a large blue 
dot on top of the existing training set data.  My data point is off the red line -- and that distance, indicated by a small blue line, represents the
\textit{residual} for the data.

		%chunk23: graph cdc data with my residual:  no chunk code:  just run chunk23.R script
		%<<chunk23, echo=F>>=
		%	
		%@

	%residual
	\begin{figure}[h!]
	\centering
		\includegraphics[height=15em]{{model3.png}}
	\caption{The small vertical blue line represents a residual:  the difference between my weight and the predicted weight from the 
		neural net predictive model.}
	\label{residual}
	\end{figure}

My blue dot represents just one point; but each data point in a testing set will have a residual.  How can we aggregate these values to obtain an 
overall picture of the accuracy of our model?  If we just added residuals, we could have a deceptive picture of accuracy:  if half of the data 
points were over estimated by the model, and half of the data points were under estimated, the sum of the residuals might be close to zero, but the 
predictions would be uniformly inaccurate.

	To adjust for the sign of the residuals, we define the \textbf{standard error} as the sum of the \textit{squares} of the residuals divided
by the number of observations.  This formula not only eliminates the issue of the positive and negative residuals, but it is nicely analogous to 
the formula for variance of a data set:

	\begin{align*}
	\text{Standard Error} &= \frac{\sum (\bar{Y} - Y)^2}{N} \\
	\text{Variance} &= \frac{\sum(x-\bar{x})^2}{n-1}
	\end{align*}	

The standard error serves nicely as a metric that will represent the accuracy of our predictive models.  Through cross-validation, we will collect
a sampling of standard errors, for both GLM and NN predictive models, and a statistical summary of these standard errors will offer a more robust 
description of a predictive model's accuracy.

%--------------------SUBSECTION #2.2: PARTS of GLM------------------------------
\subsection{The Parts of a Generalized Linear Model.}

For every single predictive model, we need to have observed data, we need to have classified the predictor and response variables.  To test the 
accuracy of any given predictive model, we need to establish how we will cross-validate the model with our data set, and we need to identify a 
statistic (such as standard error) to quantify the accuracy of a model.  With these pieces in place, we can then look at some of the details unique 
to GLM models.
	
	%-------------COEFFICIENTS------------------------
	\subsubsection{The Coefficients for a GLM formula.}
In discussing how the formula for a predictive model, I committed only
to the language `is related to' to describe the relationship between the variables.  But, in the case of a GLM predictive model, we can strengthen 
this description: the model will describe a linear relationship between the response and predictor variables.  That is, a GLM model will provide
an equation in the form of

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 \dots \beta_k X_k,$$

\noindent that relates the response variable $Y$ and the $k$ different predictor variables $X_i$.  The \textbf{model coefficients} $\beta_i$ need 
to be calculated so that our model best `fits' the observed data.   As mentioned in the previous section, the idea of `fit' is quantified by the
standard error; so the calculation of the coefficients $\beta_i$ then becomes an optimization problem -- we need to calculate $\beta_i$ so that
the standard error of the model will be minimized.  These calculations are easily carried out in R;  a very simple, two-dimensional 
example is given is Figure \#\ref{glmcoefficients}.

		%glmcoefficients
		\begin{figure}[h!]
		\centering
		\includegraphics[height=2.2in]{{regression1.jpg}} 
			<<chunk24,echo=3,background="grey80">>= 
			figure5<-data.frame(lemons=c(240,290,355,445,530),fatalities=c(15.86,15.7,15.36,15.31,14.88)) 
			glm(fatalities~lemons, data=figure5)->glm.output
			glm.output
			@
		\caption{A graph of a basic linear regression, and the output -- including coefficients $\beta_i$ -- provided by a GLM predictive 
			model fitting the data.}
		\label{glmcoefficients}
		\end{figure}
		%glmcoefficients

\noindent Notice where it says \texttt{Coefficients:} in the output: these are the values $\beta_0$ and $\beta_1$ for the model.  A defining feature of a GLM
model is that we can analyze the specific coefficients generated by a model to establish a relationship between predictor and response variables.
For example, in the model represented in Figure \#\ref{glmcoefficients}, the coefficient $\beta_1= \Sexpr{glm.output$coefficients[2]}$ indicates 
that for each metric ton of lemons imported to the US, the highway fatality rate will drop by $\Sexpr{abs(glm.output$coefficients[2])}$.

We can even determine confidence intervals for these coefficients.  If the confidence interval for a given coefficient does NOT contain zero, then 
we can state (with some degree of confidence) that there is a correlation (though not necessarily a cause-effect relationship!) between the 
variables.  (And, if you believe the correlation result from above, I've got a statistics book to sell you\dots)
  %--------IF you have time.... write more on confidence intervals for GLM coefficients!!!!

Although we will not further analyze model coefficients in this paper, their presence in GLM models can give us very useful language to describe 
the relationship between specific variables in a dataset.  

	%-------------LINK FUNCTIONS------------------------
	\subsubsection{The Link Function for a GLM model.}

In Figure \#\ref{glmcoefficients} above, it is a reasonable assumption (from the observed data) that the response variable, the highway fatality
rate, is normally distributed.  (In practice, you'd prefer more than five data points to corroborate your assumption\dots)  In this case, the 
linear relationship outlined by GLM provides an appropriate model for the response variable.  However, consider a situation where our observed 
data consists of the points an NBA team scores in a game, and whether or not they win the game.  Here, the second variable is binomially
distributed -- `win' or `loss' -- and so trying to model this variable with a linear relationship as before, we see how poorly our model represents
the data.  We want an output between 0 (definite loss) and 1 (definite victory).  The line exceeds those bounds on the ends, and in the middle the
line appears to ignore our data altogether.  This woeful model is shown in Figure \#\ref{binomialbad}.

%--------------image:  'bad' GLM model w/ binomial output and identity link

%chunk25:  code to create GLM identity model for binomial response, using NBA data
%<<chunk25,echo=3,background="grey80">>= 
%			
%@
		%binomialbad
		\begin{figure}[h!]
			\centering
			\includegraphics[height=1.8in]{{binomialbad.png}} 
		\caption{Binomial response data -- WIN or LOSE for a basketball game -- is misrepresented by a predictive model expecting a
			normally distributed response}
		\label{binomialbad}
		\end{figure}


The solution to this?  We'll add another step to the GLM model process that describes the predictor-response relationship using the 
$Y=\beta_0 + \sum \beta_i X_i$ equation:  we'll \textit{link} this linear output to a particular function that better models our specific response
variable.  For our circumstance, with the output varying between 0 and 1, we'll use a logistic function (with range 0 to 1) to `link' the linear 
output to the binomial response variable.  The results in Figure \#\ref{binomialgood}, and the improved fit over the previous results, demonstrate 
the effectiveness of using a link function in this case.  

%-------------image;  data + linear output + logistic link

		%binomialgood
		\begin{figure}[h!]
			\centering
			\includegraphics[height=1.8in]{{binomialgood.png}} 
		\caption{Binomial response data -- WIN or LOSE for a basketball game -- is represented much more meaningfully when the
			linear regression is \textit{linked} to a logistic function}
		\label{binomialgood}
		\end{figure}

Comparing the linear output in Figure \#\ref{binomialbad} and Figure \#\ref{binomialgood}, we notice that the red lines are different.  What 
happened?  In the first graph, the linear output, the red line, is designed to predict the response variable directly (through the process of
minimizing standard errors, as mentioned previously).  In the second graph, the red line is not directly representing the response variable -- that's
the job of the blue logistic graph.  Instead, the red line represents intermediate values that `link' the predictor variables to the response 
variables.  Predictor variables are mapped onto the red line via linear combination; the values on the red line are mapped onto response variables 
via the logistic equation.  The red line itself is nothing more than intermediate values -- neither predictor nor response variables --  with no 
discernable meaning in the context real-world context of the scenario.  This is why the line awkwardly slants across the data in Figure 
\#\ref{binomialgood}, compared to the line in Figure \#\ref{binomialbad}, which at least tries to approximate the responses.

Understanding these chain of mappings, from predictors to linear combinations to responses, is important to understanding GLM models.  Equally 
important is to understand that the intermediate step on that chain -- the linear combination -- has no `real-world' meaning in terms of the
context of the problem.  But it allows us to `link' to an appropriate function (in Figure \#\ref{binomialgood}, a logistic graph) to represent the
response.  Making qualitiative sense of these intermediate linear combinations can't be done without digging deeper into the math -- beyond the
scope of this paper -- since the values don't make sense in the context of the problem.  But, when the overall GLM process is understood, they are 
easily understood qualitatively -- they `link' our predictors to our responses.

Details on the linear combination aside, for other GLM predictive models, it is necessary to understand the distribution of the response variable, 
to select an appropriate \textit{link function} -- the logistic function above would not be appropriate for, say, a response with a Poisson
distribution.  We saw that for the normally distributed response variables (in Figure \#\ref{glmcoefficients}), there is no link function (or the 
link function is the trivial identity function).  For binomially distributed response variables (Figure \#\ref{binomialgood}), the link function is 
a logistic function.  For responses variables with a Poisson distribution, a log function is used as a link function. [read annotte for other links]

For these three types of scenarios -- normal, binomial, and Poisson distributions for a response variable -- the process of connecting linear 
output from the predictor variables to a link function puts the `G' in GLM:  it generalizes the linear regression methods shown in Figure \#\ref{glmcoefficients} to
encompass response variables that do not exhibit linear behavior.  A common thread for the scenarios that allow us to apply the GLM process is that the 
response distributions all belong to the \textit{exponential} family of distributions.

%--------------------SUBSECTION #2.3: PARTS of NN--------------------------------
\subsection{The Parts of a Neural Net.}

In the previous section, we described how GLM requires some extra information from us (the distribution of the response variable), but this method
also provides us with some extra information (the coefficients describing the linear relationship between the variables).  For neural nets, no 
extra information is required\dots but no extra information is given.  For GLM, we can describe some of the pieces that help us connect the 
predictors to the response:  the model coefficients, the link function.  For NN, the model can, in ways, resemble a `black box':  observed data
goes in, predictions come out.  For GLM, I described the intermediate linear output as `slightly cryptic'.  For NN, the intermediate stages go from 
`slightly cryptic' to `bafflingly abstruse to the point of opacity'.  Descriptions for what happens in between can be elusive, but I'll try\dots

\subsubsection{What is a Neural Net?}
A neural network is a system for computational that can `train' its performance (that is, improve its accuracy) from observed data.  Similar to how 
your lifelong history experiencing, say, chairs represents an observed dataset, and your brain has been `trained' on that data to know whether or 
not a given object is a chair,  a neural network can be `trained' on a given dataset and from its training it will output information about a 
pre-determined response variable.   For GLM the variables are connected using a linear combination and an appropriate link function, subject to 
specific conditions (i.e., minimizing standard error).  How does a neural net establish this connection?  How is the process of `training' a neural 
network different than how a GLM model is built from observed data?  

At its root, a neural net accomplishes this in exactly the same way as a GLM model: it combines linear combinations and link functions to connect 
predictor variables to a response.  The fundamental differences between the models: (1) a neural net does not necessarily rely on only one linear 
combination and link function to connect inputs to outputs -- there may be many! -- and (2) the linear combinations do not represent the solution 
to a optimization problem (i.e., minimizing the standard errors) as they do for GLM.  Instead, these linear combinations are the end 
product of a computionally intensive trial-and-error process guided by a pre-determined \textit{learning rule}.

\subsubsection{The Structure of a Neural Net.}
Neuroscience tells us that brain activity is dispersed across a network of interconnected neurons.  Simulating this interconnectedness by 
replicating the distrubuted structure of the neurons in a human brain is the fundamental feature of a NN predictive model.  And, by `distributed 
structure' for a NN predictive model, we mean that the results from the linear combination of the predictor variables does not directly arrive us 
to the response variables.  Instead, the results of one linear combination are then plugged into another linear combination, the results of which 
might be plugged into another\dots and, eventually, the chain of combinations (and link functions!) arrives to the response outputs.

To visualize a neural network, rather than an $xy$-graph, consider a directed graph with edges and vertices.  Each predictor variable has a vertex,
and the response variable is a vertex, also.  The predictor vertices represent a \textit{layer} of vertices, and the response vertex is, by itself,
a layer also.  All the vertices in one layer are connected to each vertex in the subsequent layer.  The number of layers, beyond the initial layers that 
contain the predictor and response variables, is a matter of choice.

%-------------NN graph!!

With the structure of a neural network established, we give values to the vertices.  Predictor variables are assigned values from an observed data
point.  Vertices in subsequent layers receive values calculated by \textit{weighted sums} of the adjacent vertices (c.f. linear combinations in GLM
models), which are then transformed via a \textit{activation function} (c.f. link functions in GLM).  Whereas the linear combination and link
function will be applied once to data in a GLM model, if we have assigned a number of layers of neurons to our NN model, this process will be
repeated before we arrive at an output for our response.  

%-----------NN graph side-by-side analogous GLM graph

Before we conclude that NN is simply `supersized GLM' owing to some of the similiarities in the above graphic, consider how we establish values for
the weights in the NN digraph, and consider the purpose of the activation function.

	\subsubsection{Training a Neural Net.}
The raw values from our observed data must be scaled to values between -1 and 1 (or between 0 and 1) before they are assigned to the vertices in 
the neural net.  Initially, the weights connecting vertices are chosen at random.  The activation functions are functions such as an inverse 
tangent or a logistic function with an output in the range of -1 to 1, ensuring values in all subsequent neuron layers, as well as the response 
value, are scaled similarly to the initial data.

The accuracy of a model with weights chosen at random is likely to be suspect.  To improve that, the a NN model can be trained.  The training
process is iterative -- each data point from our observed data is scaled and fed into the neural network.  The training algorithm will compare the
output from the neural network to the actual response value from the data, and adjust the weights as necessary.  How are the weights adjusted?  
Typically an algorithm will have a `learning rule' that stipulates how to modify the weights.  In a GLM model, there is a goal in adjusting the
parameters $\beta_i$: minimizing the standard error for the model.  For training a neural net there is a `blind' rule defined for adjusting the parameters.  Remarkably, in the end, after repeated iterations, this `blind' rule accomplishes can achieve accurate output.  But is it more accurate than GLM?  Fast forward to Section \#3 for details of a head-to-head comparison between the methods\dots

The iterative training process of the `learning rule' in a neural net is adaptable:  we do not need to specify the type of
distribution for our response (as we do with GLM).  We can introduce more data points to an existing model to better train it incrementally,
whereas GLM requires us to provide all the observed data up front to create a model.  (At least, this is true in theory -- the algorithms I am
familiar with for creating a NN model require all the data up front, also.)  The process can be `brittle' -- it is possible for a learning rule can produce accurate
models ten times in a row, but on the eleventh, the model outputs might continually diverge from the actual responses as the learning rule is iterated.  
The complexity of the NN structure means it is virtually impossible to troubleshoot why a divergence like this might arise.

When it works, it's remarkable to see a neural net build an accurate predictive model through pure computational muscle and faithful persistence to a learning rule.  When it doesn't, I'm left frustrated with an opaque black box, tweaking the structure of the NN, crossing my fingers and trying again.  It's about as close to mysticism as I'm comfortable with for mathematics -- and, understanding that, we ought to be careful what data we might analyze using a neural net.  Data that determines whether or not a patient gets a risky operation with life-and-limb in the balance?  Not wise.  Data that looks at whether or not a basketball team would win a game, based on their team statistics?  Sure!  On that note\dots





