%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%% 495 Report: comparing standard error between GLM and NN models
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------
%-------- preamble
%----------------------------------

%---set font size, options for document
	\documentclass[12pt]{article}  
%---set numbering depth
	\setcounter{secnumdepth}{2}
%---get packages!
	%---for math notations
	\usepackage{amsmath}
	%---for including graphics	
	\usepackage{graphicx}
	%---for hyperlinks
	\usepackage{hyperref}
	%---hyperlink format
	\hypersetup{colorlinks=true,urlcolor=blue}

%---set graphicspath
	\graphicspath{
		{img/}
		}
	
%%%%%%%%%%%%%%%% DOCUMENT ENVIRONMENT %%%%%%%%%%%%%%%%%%%
\begin{document}

%---top matter
	\title{Generalized Linear Models and Neural Nets: Comparison Between Methods for Predictive Analysis}
	\author{William R. Johnson}
	%%%%other title commands??  advisor?  
	\date{12 February 2018}
	\maketitle

%---abstract
	\renewcommand{\abstractname}{Abstract:}
	\begin{abstract}
		Whether a \href{https://fivethirtyeight.com/features/oscars-2018-early-predictions-nominations/}{fluffy article about primetime 
awards programs on the website fivethirtyeight.com}, or deep within the analytical 
department of a multi-billion dollar, multinational corporations, predictive analysis is a critical part of our world in the age of ``Big Data".  
In this paper, I will examine two types of predicitive analysis, Generalized Linear Models (GLM) and Neural Nets (NN).  I will offer a cursory 
overview of the mathematics behind each type of model, and using the R statistical software, I will analyze the accuracy of both GLM and NN 
predictive models, as well as the time needed to construct each model. I will consider an approximation of the standard error as a measure of model 
accuracy, and I will compute these approximations using 50-fold cross-validation.  The models will be fit to two different datasets -- one with a 
normally distributed response variable, and one with a binomially distributed response.  The results -- estimated standard errors and processing 
time -- will be analyzed across the different model types, the different response variables, and different sizes of training sets.
	\end{abstract}
	\vfill\eject


%------------------------------
%-------------body
%------------------------------

%---------------------------------------------------------------------------------------------------------------
%---------------SECTION #1: INTRO, citation of GLM/NN in scholarly works, overview of my study------------------
%---------------------------------------------------------------------------------------------------------------

	\section{Intro to Predictive Analysis.}
		We live in the age of `Big Data'.  Using that data to measure, observe, and predict has great value:  economically, culturally, 
socially.  It's easy to say that this prediction is done by an app -- highlight data, point, click, ta-da!  You can find \href{https://www.google.com/maps/dir/Chicago,+Illinois/Champagne+Beach,+Vanuatu/@10.9584518,-169.5679877,3z/data=!4m13!4m12!1m5!1m1!1s0x880e2c3cd0f4cbed:0xafe0a6ad09c0c000!2m2!1d-87.6297982!2d41.8781136!1m5!1m1!1s0x6ef24c13ce5a7303:0x5807cbc94589a4bb!2m2!1d167.1203814!2d-15.1437919}{estimated travel time}, 
\href{https://www.amazon.com/Statistics-Dummies-Math-Science/dp/1119293529/ref=sr_1_1?ie=UTF8&qid=1518456852&sr=8-1&keywords=statistics+for+dummies+2017}{`Related materials' in consumer recommendations}, and \href{http://www.espn.com/mens-college-basketball/game?gameId=400986579}{predicted outcome of sporting events} with a few simple taps on your 
phone.  But what's the mathematical machinery behind these predictions?  Are all the predictions calculated in a similar manner?  How accurate is a
prediction?  How long would it take to compute a prediction, for a given set of data?  All of these questions are critical to ensure that predictive models 
remain practical and functional in our `Big Data' world.

Now, I will not discuss the specific predictive methods for proprietary applications, like the aforementioned travel predictions from Google or 
suggested product recommendations from Amazon.  But I will offer a cursory examination of two major mathematical methods for predictive analysis:
Generalized Linear Models (GLM), and Neural Networks (NN).  Both methods allow us to `train' mathematical models on an existing data set, which can
subsequently be used to predict future outcomes, or \textbf{response variables}.  Figure \#1 contains two graphs that illustrate, graphically, these 
two types of mathematical models for a simple data set.

Given a sample of heights and weights from 20000 individuals in the United States collected from a 2015 survey conducted by the Centers for Disease
Control, the red lines indicate the predicted response variables from mathematical models trained on this data.  

\begin{figure}[h!]
\centering
\includegraphics[height=1.8in]{{model1.png}}
\includegraphics[height=1.8in]{{model2.png}}
\caption{Predictive models for height and weight data, trained using GLM methods (left), and NN methods (right)}
\end{figure}

These are simple examples; the data set has only two variables, and intuitively most people have an understanding of how these variables relate 
without appealing to mathematics.  But the underlying principle -- that the data can `train' a predictive mathematical model -- is one that we will
observe and analze in many other contexts over the course of this paper.

%---------------SUBSECTION #1.1: GLM in scholarly works---------------------

	\subsection{GLM in Mathematical Literature.}
		For many scientific breakthroughs, there is no one, clear, defining `AHA!' moment where a new idea was discovered from scratch. For
GLM, however, the is a moment that approximates that: the publication in 1972, by Nelder and Wedderburn, of the paper `Generalized Linear Models'
in the \textit{Journal of the Royal Statistical Society}.  This often-cited work clearly laid out many of the ideas that are central to the study 
of GLMs today.  On the back of this work, and with the rise of widely-available statistical software to calculate GLM models such as R and SPSS, 
these models have been widely applied across the scientific literature in service of predictive analysis.  For instance, GLM has utility in
analyzing systems as varied as 
\href{https://www.sciencedirect.com/science/article/pii/S037877961630222X}{hydropower generation}, 
\href{https://www.sciencedirect.com/science/article/pii/S1470160X17306271}{habitat selection for small mammals}, and 
\href{https://www.sciencedirect.com/science/article/pii/S0167668715303358}{insurance claims}.  

Certainly, if GLM is used to obtain predictions of economically or ecologically important data, as suggested in the links above, we would want to 
ensure that the predictions are accurate. We will demonstrate in this paper how to estimate the standard error for a GLM model, which can be 
used to draw up confidence intervals and quantify the accuracy of a prediction.  Having these estimates for accuracy can be vital in determining
whether or not a predictive model is a valid basis for making policy decisions that carry great economic or environmental risk.

%---------------SUBSECTION #1.2: NN in scholarly works----------------------

	\subsection{NN in Mathematical Literature.}
		From the earliest work on \textit{threshold logic} by McCulloch and Pitts in 1943, to the explosion of machine learning algorithms
in use today in the age of `Big Data', the idea that the architecture of computing systems could be inspired by the architecture of the human brain
and its neural networks has maintained a presence in the scientific literature.  With the dramatic rise in computational power in the last 
generation along with statistical software such as R and SPSS, interest in neural networks has flourished, along with its place in scientific
research.  We can see examples of current research using neural nets to analyze topics as diverse as 
\href{https://www.sciencedirect.com/science/article/pii/S1877050918300656}{the gender of Russian authors},
\href{https://www.sciencedirect.com/science/article/pii/S2468203916300024}{the surface radiation in the Sundarban forest in India}, and 
\href{https://www.sciencedirect.com/science/article/pii/S1877050917303617}{routes through the city of Zhengzhou chosen by tourists}.
		
A distinct characteristic of neural nets is that of \textit{machine learning} -- given more data to `train' the model, it can improve the accuracy
of the response variable.  However, adding more data to a neural network requires more computational muscle, and for large or complex datasets,
it can require significantly more time to process.  So, similar to GLM, it will help us to have a means to check how accurate our predictions are,
so we can determine whether or not the added data and accuracy is worth the extra time needed for processing.  As with GLM, our means of assessing
accuracy in this paper will be to approximate the standard error for a given NN model.

%---------------SUBSECTION #1.3: Predictive models and this paper-----------


	\subsection{Methodolgy.}

	$$professor's suggestion:  move \S 1.3 to chapter 3.$$
$$My suggestion:  replace this with \S1.3 on accuracy of predictive models in literature??$$

		For this paper, I will train both GLM and NN models using two pre-selected datasets.  As mentioned above, for each type of
predictive model, we can approximate the standard error for each model, quantifying for us the accuracy of each model.  We can then compare the 
standard errors between the two models, hopefully drawing some conclusions about the relative accuracy of one model compared to the other.

The datasets that will be considered are a listing of various attributes for nearly 54 000 diamonds, and team statistics for nearly 3000 
basketball games from the NBA.  The diamonds data is provided without source as part of the `ggplot2' package for the R statistical software, while
the gamelog dataset was retrieved from the website \href{https://www.basketball-reference.com}{Basketball-Reference.com}.  The diamonds dataset includes 
10 variables for each of the diamonds, while the gamelog dataset looks at 21 statistics for each basketball game.  When creating predictive models, we will 
consider the quantitative variable \textit{price} as our response variable for the diamonds dataset, and the categorical variable \textit{WL} (whether a team won 
or lost a basketball game) as our response variable for the gamelog dataset.  A motivation for working with these two datasets is the fact that the  
response variables for each dataset are distributed differently, as illustrated in the graphs in Figure \#2.

\begin{figure}[h!]
\includegraphics[height=1.5in]{{graph1.png}}
\includegraphics[height=1.5in]{{graph2.png}}
\caption{Distributions for the response variables in DIAMONDS dataset (left), and the GAMELOG dataset (right)}
\end{figure}

We can easily train calculate a GLM or NN model using an entire dataset, and then compare the predicted results from these models against the
actual response variables.  But the weakness of this method is that we are not introducing any new data for testing accuracy.  Instead, dividing 
the data into disjoint `train' and `test' subsets gives us a better idea of how accurate a predictive model would be given new, independent data.
Carrying out this `train-test' process once is prone to great variability, depending on the particular choice of dataset selected to train the 
model.  Instead, repeating this `train-test' process a number of times across repeated random samples of the dataset will give a better
picture of the accuracy for a predictive model.  This repeated resampling and model testing is known as \textit{cross validation}, and we will 
carry it out 50 times (50-fold cross validation) for both GLM and NN predictive models with both of our datasets.

In addition, the size of the `train' subset would likely affect the accuracy of a given model.  With a larger training set, we would anticipate
that a predictive model would be more accurate.  However, as alluded to above, the larger training set also will cost us in terms of processing
time.  To examine how the size of the training set affects model accuracy (and processing time), we will run each instance of 50-fold cross
validation three times, with training sets of size 500, 1000, and 2000.  

To measure accuracy for each of the models, we will calculate the observed standard error of the test subset.  The computations will be carried out
using the open-source R statistical software.

%------------------------------------------------------------------------------------------
%----------- SECTION #2: THE PARTS OF THE PREDICTIVE MODELS
%------------------------------------------------------------------------------------------

	\section{The Parts of the Predictive Models.}
In the first section of this paper, I presented two side-by-side graphs, one representing a GLM predictive model, and the other representing a NN
predictive model.  But I presented the two graphs without any description of how they were created, other than alluding to the use of statistical
computer software to carry out the computations.  The computer certainly makes the process of model creation easier; but I can't say ``Hey, R, make
me a GLM model, please!"  (Although it's not too far off, frankly\dots)  Even if the computer is doing the heavy lifting, I need to understand the
fundamental parts of a GLM model, and I need to understand some critical attributes of my data before a workable predictive model is created.  In 
this section, without wading too deeply into theoretical matters, we'll look at the parts of a predictive model, so we're able to understand the 
information we're responsible for feeding into the computer before it does its 1's and 0's thing and returns us a predictive model.  First, we'll 
break down general concepts that we'll find in any predictive model, and in the second and third sections we'll consider details specific to GLM and
NN models.

%-------------------SUBSECTION #2.1:  PARTS of a PREDICTIVE MODEL-----------------

	\subsection{The General Parts of a Predictive Model.}
	In the first chapter, in Figure \#1, we saw two different predictive models:  one created using GLM, one from NN.  Though the two models
have clear points of distinction, we can also identify some similarities.  These similarities represent some of the fundamental parts in any
predictive model.  Here, we aim to introduce precise terminology that we can use to describe these similarities, and that can help us outline the
goals of our predictive model analysis for this paper in a clear, unambiguous manner.

	\subsubsection{Observed Data.}
	The first thing, before you can even consider a predictive model:  we need to have data to draw a prediction from.  No data, no model.  And,
of course, we want our model grounded in reality -- so it's important to verify that your data consists of well sourced observations before 
beginning to construct any model.  There's a well-known phrase among programmers:  `Garbage in, garbage out.'  Your predictive model is only as 
good as your data.  Make sure the process of model building starts atop a legitimate source of observed data.

	For our analysis in this paper, I will consider two data sets:  a collection of information on nearly 54,000 diamonds, and statistics for 
nearly 3,000 NBA basketball games.  The headers for both data sets are given below.

%%%%%%5----header for diamonds, gamelog

	\subsubsection{Variables and Formula.}
	Once you have data sets to work from, it is necessary to identify and classify the variables each data set contains.  In particular, we 
need to classify the variables for our observed data in one of three different ways: as a \textbf{predictor variable}, a \textbf{response variable},
or a variable to ignore in our model.  

Below is a header for the full data set from the 2015 CDC survey used in Figure \#1 in the previous chapter.  This data set has nine different 
variables, and we designated \texttt{height} as the predictive variable, and \texttt{weight} as the response variable.  We ignored the other seven 
variables for this model.

	Is it necessary to ignore all those othe seven variables? Not at all -- in Figure \#?? below, three of the variables are represented in a
graph:  \texttt{height} and \texttt{age} are plotted on the $X-$ and $Y-$axes of the graph, and \texttt{weight} is represented by the colors in the
plots.  Squinting just right, you might be able to discern the colors darkening as you move to the left or upwards in the graph.  Put another way,
the age and height (the left/upwards in the graph) can help us predict the weight (color).  Or, \texttt{height} and \texttt{age} are our predictor
variables, and \texttt{weight} is our response variable.  Again we ignore the other variables in the data set.  (The choice here to ignore most of 
the variables is solely because visualizing these models is only feasible when we have two or three variables in consideration.)

%-----------figure with raster plot of CDC data, and CDC model

	After identifying the predictor and response variables, we can also write a formula that describes this relationship between the variables.
For the example in Figure \#XXX above, we would write \texttt{weight \~ height + age}, the left-hand argument signifying our response variable, and
the right-hand sum comprised of the predictor variables.  (Could we add exponents or other operations to these terms?  Sure\dots but that's for
another paper.)  Read aloud, we might say ``Weight is related to height and age". \textit{Is related to} is fairly vague language, but as we'll 
see later on, the nature of the relationship depends on a number of other factors -- so we can offer a stronger characterization, here.

	For our analysis, when considering the \texttt{diamonds} dataset, we will designate \texttt{price} as the response variable, and all nine
other variables will be predictor variables.  The resulting formula:

\begin*{equation}
\texttt{price \~ carat + cut + color + clarity + depth + table + x + y + z} 
\end*{equation}

can be abbreviated as \texttt{price \~ .}, with the dot indicating that all remaining variables are predictors.  For our gamelog data, we will use
the formula \texttt{WL \~ .}, hoping that all the measured statistics can predict the response \texttt{WL} -- that is, we hope the stats can predict
whether or not a team won a basketball game.

	\subsubsection{Cross Validation.}
With our observed data set, and a formula that outlines the relationship we posit in our data, we could begin to create a predictive model using
our entire dataset.  Furthermore, we could take this predictive model and test it: we could compare the model's predicted values for the response
variable against the actual response variables in the data.  The weakness of this method is that we are not introducing any new data for testing 
accuracy.  

A solution to this issue is to divide the data into disjoint subsets, called the \textbf{training set} and \textbf{testing set}.  From here, we 
create the model using the training set, and we test the model for accuracy using the testing set.  Separating out this process gives us a better 
idea of how accurate a predictive model would be given new, independent data.  However, carrying out this `train-test' splitting of the data just 
once is prone to great variability, depending on the particular choice of dataset selected to train the model.  Instead, repeating this `train-test' 
splitting a number of times, repeatedly taking random samples from the data to select training and testing sets, will give a better picture of the 
accuracy for a predictive model.  This repeated resampling and model testing is known as \textit{cross validation}.

The ratio of the sizes of the train and test subsets can be adjusted, though a larger training subset will likely exhibit less variability across
repeated resamplings.  A common ratio of the size of training and testing sets is 70-30.  In our analysis, we will hold a 10-1 ratio of the
training and testing sets.  Also, we will resample 50 times to create 50 different training and testing sets (50-fold cross validation).

	\subsubsection{Residuals and Standard Error.}
	Above we alluded to how we can consider accuracy for a predictive model: comparing the model's predicted values for the response
variable against the actual response variables in the data.  The difference between these values, the predicted and the actual response, is called
the \textbf{residual}.  Each data point in our testing set will have a residual.  How can we aggregate these values to obtain an overall picture of
the accuracy of our model?  If we just added residuals, we could have a deceptive picture of accuracy:  if half of the data points were over
estimated by the model, and half of the data points were under estimated, the sum of the residuals might be close to zero, but the predictions
would be uniformly inaccurate.

	To adjust for the sign of the residuals, we define the \textbf{standard error} as the sum of the \textit{squares} of the residuals.  This
formula not only eliminates the issue of the positive and negative residuals, but it is nicely analogous to the summation in the formula for
standard deviation for a data set:

\begin*{equation}
$SE = \sum (\bar{Y} - Y)^2$
$SD = \sqrt{\frac{\sum(x-\bar{x})^2}{n-1}}$
\end*{equation}	

The standard error serves nicely as a metric that will represent the accuracy of our predictive models.  Through cross-validation, we will collect
a sampling of standard errors, and a statistical summary of these standard errors will offer a more robust description of a predictive model's 
accuracy.

%--------------------SUBSECTION #2.2: PARTS of GLM------------------------------

\subsection{The Parts of a Generalized Linear Model.}

For every single predictive model, we need to have observed data, we need to have classified the predictor and response variables, and use
these variables to draw up a formula for the model.  To test the accuracy of any given predictive model, we need to establish how we will 
cross-validate the model with our data set, and we need to identify a statistic (such as standard error) to quantify the accuracy of a model.  
With these pieces in place, we can then look at some of the details unique to GLM models.

\subsubsection{The Coefficients for a GLM formula.}
In discussing how the formula for a predictive model takes the form \texttt{response variable $\sim$ predictor variables}, I committed only
to the language `is related to' to describe the relationship between the variables.  But, in the case of a GLM predictive model, we can strengthen 
this description: the model will describe a linear relationship between the response and predictor variables.  That is, a GLM model will provide
an equation in the form of

\begin*{equation}
$Y = \beta_0 + \beta_1 * X_1 + \beta_2 * X_2 \dots \beta_k * X_k,$
\end*{equation}

that relates the response variable $Y$ and the $k$ different predictor variables $X_i$.  The \textbf{model coefficients} $\beta_i$ need to be
calculated so that our model best `fits' the observed data.  We can see these pieces of a GLM model in a simple, two-variable linear regression
model:

\begin{figure}[h!]
\includegraphics[height=1.5in]{{regression1.jpg}}
\caption{A basic linear regression with dataset, predictor and response variables, and linear model.}
\end{figure}

Here we can easily see three of the critical parts of a predictive model:  the observed data (the red dots in the graph), the predictor variable 
(tonnage of lemon imports), and the response variable (highway fatality rate).  In the process of building a GLM model, coefficients $\beta_0$ and
$\beta_1$ were calculated so that the standard errors (the sum of the residuals squared) is minimized, providing us with a linear relationship 
between the variables that is represented by the line on the graph.

When training a GLM model, the specific values for these coefficients can be obtained, and we can determine confidence intervals for the values of
these coefficients.  Although we will not further analyze model coefficients in this paper, their presence in GLM models can give us very useful
language to describe the relationship between variables.  For instance, looking at the coefficients for the model shown in Figure \#?? above,
we see that for every XXX of lemon imports, the highway fatality rate decreases by YYY.

%---insert output from GLM

If you believe that result, I've got a statistics book to sell you\dots

\subsubsection{The Link Function for a GLM model.}

In figure \#4 above, it is a reasonable assumption (from the observed data) that the response variable, the highway fatality rate, is normally 
distributed.  (In practice, you'd prefer more than five data points to corroborate your assumption\dots)  In this case, the linear relationship
outlined by GLM provides an appropriate model for the response variable.  However, consider a situation where our observed data consists of the 
points an NBA team scores in a game, and whether or not they win the game.  Here, the second variable is binomially distributed -- `win' or `loss' 
-- and so trying to model this variable with a linear relationship and an equation with an output that spans the real number line has a fundamental
flaw:  we want an output between 0 (definite loss) and 1 (definite victory).

%--------------image:  'bad' GLM model w/ binomial output and identity link

The solution to this?  We'll add another step to the GLM model process that describes the predictor-response relationship using the 
$Y=\beta_0 + \sum \beta_i*X_i$ equation:  we'll \textit{link} this linear output to a particular function that better models our specific response
variable.  For our circumstance, with the output varying between 0 and 1, we'll use a logistic function (with range 0 to 1) to `link' the linear 
output to the binomial response variable.  

%-------------image;  data + linear output + logistic link

In general, for GLM predictive models, it is necessary to understand the distribution of the response variable, and then select a
\textit{link function} that appropriately connects the linear output from the predictor variables to the response variable.  The particular
link function to be chosen is (usually) determined by the distribution of the response variable; for the normally distributed response variables,
there is no link function (or the link function is the trivial identity function).  For binomially distributed response variables, the link 
function is a logistic function.  For responses variables with a Poisson distribution, a log function is used as a link function.

For these three types of scenarios -- normal, binomial, and Poisson distributions for a response variable -- the process of connecting linear 
output from the predictor variables to a link function puts the `G' in GLM:  it generalizes the linear regression methods shown in Figure \#??? to
encompass response variables that do not exhibit linear behavior.  A common thread for the scenarios that allow us to apply the GLM proess?  The 
response distributions all belong to the \textit{exponential} family of distributions.

%--------------------SUBSECTION #2.3: PARTS of NN--------------------------------

\subsection{The Parts of a Neural Net.}

In the previous section, we described how GLM requires some extra information from us (the distribution of the response variable), but this method
also provides us with some extra information (the coefficients describing the linear relationship between the variables).  For neural nets, no 
extra information is required\dots but no extra information is given.  For GLM, we can describe some of the pieces that help us connect the 
predictors to the response:  the model coefficients, the link function.  For NN, the model can, in ways, resemble a `black box':  observed data
goes in, predictions come out.  Descriptions for what happens in between can be elusive, but I'll try\dots

\subsubsection{What is a Neural Net?}

To offer a succinct definition of a neural net and how it works would be as simple as explaining how you know a chair when you see 
one.  That is, your experience with all different forms of chairs, all the different styles of chairs, and all the different ways someone might use
a chair allows you to (fairly definitely) state if something is a chair or not.  To use language more similar to those referencing mathematical
models, your lifelong history experiencing chairs is a observed dataset that has `trained' you to know whether or not a given object is a chair.  
In a similar vein, if a neural net is `trained' on a given dataset, it can output information about a pre-determined response variable.


....NN *is* linear combinations and `link' functions.  But it's iterated, data-point by data-point.

....data must be scaled to handle the link functions
....we can add 'neurons', or more iterations of the linear combination + link function process to connect predictors to response
....adjusting weights (or linear combinations) isn't to minimize a specific error metric (a la GLM)... it's simply an adjustment, since
    model output != expected output.  repeated adjustments makes the whoel thing more accurate.... even if one particular adjustment doesn't
    have a plainly evident goal.  it has a plainly evident cause -- output != expected output -- but not a plainly evident goal.  Yet, repeated
	'blind; adjustments gives the model predictive value.  !!

...data must be scaled!! 'link function' keeps neuron values between 0 and 1, so initial and output data needs to be scaled.

...NN can diverge?

...no need to specify distribution!  model is 'blind', just give it data, and let 'er go!

....trainable!  each new data point can train the model:  you don't have to re-train the whole model with N+1 data points.  just adjust the
N-calibrated model with 1 new data point.

%--------------for now!!
\end{document}

\section{Topics for Further Study.}
Having completed a very basic side-by-side comparison of GLM and Neural Net predictive methods for data sets, we can begin to focus on some 
questions that might offer opportunities for further exploration.  For instance....



%%%bibliography, etc

\end{document}
