%----------495 Report, Chapter 1----------

%---------------------------------------------------------------------------------------------------------------
%---------------SECTION #1: INTRO, citation of GLM/NN in scholarly works, overview of my study------------------
%---------------------------------------------------------------------------------------------------------------

	\section{Intro to Predictive Analysis.}
		We live in the age of `Big Data'.  Using that data to measure, observe, and predict has great value:  economically, culturally, 
socially.  It's easy to say that this prediction is done by an app -- highlight data, point, click, ta-da!  You can find \href{https://www.google.com/maps/dir/Chicago,+Illinois/Champagne+Beach,+Vanuatu/@10.9584518,-169.5679877,3z/data=!4m13!4m12!1m5!1m1!1s0x880e2c3cd0f4cbed:0xafe0a6ad09c0c000!2m2!1d-87.6297982!2d41.8781136!1m5!1m1!1s0x6ef24c13ce5a7303:0x5807cbc94589a4bb!2m2!1d167.1203814!2d-15.1437919}{estimated travel time}, 
\href{https://www.amazon.com/Statistics-Dummies-Math-Science/dp/1119293529/ref=sr_1_1?ie=UTF8&qid=1518456852&sr=8-1&keywords=statistics+for+dummies+2017}{`Related materials' in consumer recommendations}, and \href{http://www.espn.com/mens-college-basketball/game?gameId=400986579}{predicted outcome of sporting events} with a few simple taps on your 
phone.  But what's the mathematical machinery behind these predictions?  Are all the predictions calculated in a similar manner?  How accurate is a
prediction?  How long would it take to compute a prediction, for a given set of data?  All of these questions are critical to ensure that predictive models 
remain practical and functional in our `Big Data' world.

Now, I will not discuss the specific predictive methods for proprietary applications, like the aforementioned travel predictions from Google or 
suggested product recommendations from Amazon.  But I will offer a cursory examination of two major mathematical methods for predictive analysis:
Generalized Linear Models (GLM), and Neural Networks (NN).  Both methods allow us to `train' mathematical models on an \textit{observed data set}, which can subsequently be used to predict future outcomes, or \textbf{response variables} .  Figure \#\ref{intromodels} contains two graphs that illustrate, graphically, these two types of mathematical models for a simple data set.

Given a sample of heights and weights from 20000 individuals in the United States collected from a 2015 survey conducted by the Centers for Disease
Control, the red lines indicate the predicted response variables from mathematical models trained on this data.  
		
		%intromodels
		\begin{figure}[h!]
		\centering
		\includegraphics[height=1.8in]{{model1.png}}
		\includegraphics[height=1.8in]{{model2.png}}
		\caption{Predictive models for height and weight data, trained using GLM methods (left), and NN methods (right)}
		\label{intromodels}
		\end{figure}
		%intromodels

These are simple examples; the data set has only two variables, and intuitively most people have an understanding of how these variables relate 
without appealing to mathematics.  But the underlying principle -- that the data can `train' a predictive mathematical model -- is one that we will
observe and analyze in many other contexts over the course of this paper.

%---------------SUBSECTION #1.1: GLM in scholarly works---------------------

	\subsection{GLM in Mathematical Literature.}
		For many scientific breakthroughs, there is no one, clear, defining `AHA!' moment where a new idea was discovered from scratch. For
GLM, however, the is a moment that approximates that: the publication in 1972, by Nelder and Wedderburn, of the paper `Generalized Linear Models'
in the \textit{Journal of the Royal Statistical Society}.  This often-cited work clearly laid out many of the ideas that are central to the study 
of GLMs today.  On the back of this work, and with the rise of widely-available statistical software to calculate GLM models such as R and SPSS, 
these models have been widely applied across the scientific literature in service of predictive analysis.  For instance, GLM has utility in
analyzing systems as varied as 
\href{https://www.sciencedirect.com/science/article/pii/S037877961630222X}{hydropower generation}, 
\href{https://www.sciencedirect.com/science/article/pii/S1470160X17306271}{habitat selection for small mammals}, and 
\href{https://www.sciencedirect.com/science/article/pii/S0167668715303358}{insurance claims}.  

Certainly, if GLM is used to obtain predictions of economically or ecologically important data, as suggested in the links above, we would want to 
ensure that the predictions are accurate. We will demonstrate in this paper how to estimate the standard error for a GLM model, which can be 
used to draw up confidence intervals and quantify the accuracy of a prediction.  Having these estimates for accuracy can be vital in determining
whether or not a predictive model is a valid basis for making policy decisions that carry great economic or environmental risk.

%---------------SUBSECTION #1.2: NN in scholarly works----------------------

	\subsection{NN in Mathematical Literature.}
		From the earliest work on \textit{threshold logic} by McCulloch and Pitts in 1943, to the explosion of machine learning algorithms
in use today in the age of `Big Data', the idea that the architecture of computing systems could be inspired by the architecture of the human brain
and its neural networks has maintained a presence in the scientific literature.  With the dramatic rise in computational power in the last 
generation along with statistical software such as R and SPSS, interest in neural networks has flourished, along with its place in scientific
research.  We can see examples of current research using neural nets to analyze topics as diverse as 
\href{https://www.sciencedirect.com/science/article/pii/S1877050918300656}{the gender of Russian authors},
\href{https://www.sciencedirect.com/science/article/pii/S2468203916300024}{the surface radiation in the Sundarban forest in India}, and 
\href{https://www.sciencedirect.com/science/article/pii/S1877050917303617}{routes through the city of Zhengzhou chosen by tourists}.
		
A distinct characteristic of neural nets is that of \textit{machine learning} -- given more data to `train' the model, it can improve the accuracy
of the response variable.  However, adding more data to a neural network requires more computational muscle, and for large or complex datasets,
it can require significantly more time to process.  So, similar to GLM, it will help us to have a means to check how accurate our predictions are,
so we can determine whether or not the added data and accuracy is worth the extra time needed for processing.

%---------------SUBSECTION #1.3: Accuracy of predictive models-----------

	\subsection{Accuracy of Predictive Models.}
	As alluded to above, testing the accuracy of a predictive model is of critical, given the numerous ways in which these models are applied to real-world applications of great economic, social, or environment importance.  A common test for accuracy with a predictive model is the standard error, as a number of scientific research articles have analyzed how to estimate this quantity...

	Above we alluded to how we can consider accuracy for a predictive model: comparing the model's predicted values for the response
variable against the actual response variables in the data.  The difference between these values, the predicted and the actual response, is called
the \textbf{residual}.  Each data point in our testing set will have a residual.  How can we aggregate these values to obtain an overall picture of
the accuracy of our model?  If we just added residuals, we could have a deceptive picture of accuracy:  if half of the data points were over
estimated by the model, and half of the data points were under estimated, the sum of the residuals might be close to zero, but the predictions
would be uniformly inaccurate.

	To adjust for the sign of the residuals, we define the \textbf{standard error} as the sum of the \textit{squares} of the residuals.  This
formula not only eliminates the issue of the positive and negative residuals, but it is nicely analogous to the summation in the formula for
standard deviation for a data set:

\begin*{equation}
$SE = \sum (\bar{Y} - Y)^2$
$SD = \sqrt{\frac{\sum(x-\bar{x})^2}{n-1}}$
\end*{equation}	

The standard error serves nicely as a metric that will represent the accuracy of our predictive models.  Through cross-validation, we will collect
a sampling of standard errors, and a statistical summary of these standard errors will offer a more robust description of a predictive model's 
accuracy.
