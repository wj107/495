%---------------495 Report, Chapter 3------------------

%----------------------------------------------------------------------
%----------------SECTION 3.4: Results for GAMELOG 
%----------------------------------------------------------------------

\subsection{Results for predictive models built from \texttt{gamelog}.}
After calling the \texttt{se.estimate} function for the subsets of the \texttt{gamelog} data, we have a sampling of 300 standard errors, 
after the script built and tested 300 predictive models. In Figure \#\ref{gameloggraph}, we organize this sampling according to the type of 
predictive model (GLM or NN) and the size of the testing subset for the model (50, 100, or 200).

	%gameloggraph
	\begin{figure}[h!]
	\centering
		\includegraphics[width=5in]{{gameloggraph.png}}
	\caption{Graphical summary of standard errors built from \texttt{gamelog} dataset}
	\label{gameloggraph}
	\end{figure}

Surprisingly, the first observation is that neural net predictive models are notably \textbf{less} accurate in this example.  Across all three
sample sizes, the GLM models showed better accuracy.  The results are summarized in Table \#4.

	<<chunk311, echo=F, background="grey80", size="footnotesize", comment=NA>>=
	require(knitr)
	load("R/results_gamelog.RData")
#-----find mean across GLM, NN and test sizes 
	tapply(results.gamelog[,4],results.gamelog[,2:3],mean)->mean.table.gamelog
#-----re-write grouped data in 3-column data frame.  there's an  easier way... I can't find it easily.
	mean.table.gamelog<-data.frame(TEST.SIZE=rownames(mean.table.gamelog),GLM=mean.table.gamelog[,1],NN=mean.table.gamelog[,2],row.names=NULL)
#-----create table, center alignment, w/title
	kable(mean.table.gamelog,format="latex", col.names=c("Size of Testing Set","Mean of GLM Std. Error","Mean of NN Std. Error"),
	        caption="Averages for Standard Errors for predictive models with varying sizes of test sets",align="c") %>% 
	  kable_styling(latex_options="scale_down")    ###why can't I do 'striped'  or 'booktabs'  ????
	@

Notice too that the improvement in the gains in accuracy -- the tendency of the dots to cluster -- is negligible across the three sample sizes.  The standard deviation of the standard errors across each of the tests illustrates this, as shown in Table \#5.

	<<chunk312, echo=F, background="grey80", size="footnotesize", comment=NA>>=
#-----find sd across GLM, NN and test sizes 
	tapply(results.gamelog[,4],results.gamelog[,2:3],sd)->sd.table.gamelog
#-----re-write grouped data in 3-column data frame.  there's an  easier way... I can't find it easily.
	sd.table.gamelog<-data.frame(TEST.SIZE=rownames(sd.table.gamelog),GLM=sd.table.gamelog[,1],NN=sd.table.gamelog[,2],row.names=NULL)
#-----create table, center alignment, w/title
	kable(sd.table.gamelog,format="latex", col.names=c("Size of Testing Set","Std. Dev. of GLM Std. Error","Std. Dev. of NN Std. Error"),
	        caption="Variation for Standard Errors for predictive models with varying sizes of test sets",align="c") %>% 
	  kable_styling(latex_options="scale_down")    ###why can't I do 'striped'  or 'booktabs'  ????
	@

Notice something from these values:  unlike the first scenario, with the \texttt{diamonds} data, the variation decreases in each case.  In other
words, for the \texttt{gamelog} data, for a categorical response variable, the predictive models become more consistent as more data is added to the test and train sets.  Is this tendency only present when categorical data is considered?  That would make a terrific question for further research.

Our last comparison, of the build times, is summarized in Table \#6.

 <<chunk313, echo=F, background="grey80", comment=NA>>=
#-----find mean, max of build time across GLM, NN, and test sizes
	 tapply(results.gamelog[,5],results.gamelog[,2:3],mean)->build.time1
   tapply(results.gamelog[,5],results.gamelog[,2:3],max)->build.time2
#-----combine summarized data and clean
   data.frame(cbind(build.time1,build.time2),row.names=NULL)->build.time
   build.time<-cbind(TEST.SIZE=rownames(build.time),build.time)
#-----create table, center alignment, w/title
	kable(build.time,format="latex",col.names=c("Size of Testing Set","GLM","NN","GLM","NN"),
	        caption="Build Times for predictive models with varying sizes of test sets",align="c") %>%
	  add_header_above(c("","Mean Build Time"=2,"Max Build Time"=2)) %>%
	  kable_styling(latex_options="scale_down")
		@

Once again, the difference in build time between the two types of models is very clearly apparent.  The NN models are significantly more costly, in terms of time, to construct than GLM models.  This trend, as well as the size of the discrepancy -- several orders of magnitude -- is consistent whether we're studying a quantitative response variable or a categorical response.  It seems to be a conclusive takeaway from our comparison: neural net models are significantly more time-intensive to construct.

But that said, accuracy-wise, there's a lot of conflicting information:  NN is more accurate for the quantitative response, but GLM is more
accurate for the categorical response.  For quantitative responses, more data yields more accuracy; but for categorical responses, more data yields more consistency (if not accuracy).  Do these trends hold in all cases?  Sounds like topics for future research and analysis!  On that note\dots
