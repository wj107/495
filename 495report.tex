%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%% 495 Report: comparing standard error between GLM and NN models
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%---set font size, options for document
	\documentclass[12pt]{article}  

%---set conditions in the preamble
	\setcounter{secnumdepth}{2}
%---get packages!
	%---for math notations
	\usepackage{amsmath}
	%---for including graphics	
	\usepackage{graphicx}
	%---for hyperlinks
	\usepackage{hyperref}
	\hypersetup{colorlinks=true,urlcolor=blue}


%---set graphicspath
	\graphicspath{
		{img/}
		}
	
%%%%%%%%%%%%%%%% DOCUMENT ENVIRONMENT %%%%%%%%%%%%%%%%%%%
\begin{document}

%---top matter
	\title{Generalized Linear Models and Neural Nets: Comparison Between Methods for Predictive Analysis}
	\author{William R. Johnson}
	%%%%other title commands??  advisor?  
	\date{12 February 2018}
	\maketitle

%---abstract
	\renewcommand{\abstractname}{Abstract:}
	\begin{abstract}
		Whether a \href{https://fivethirtyeight.com/features/oscars-2018-early-predictions-nominations/}{fluffy article about primetime 
awards programs on the website fivethirtyeight.com}, or deep within the analytical 
department of a multi-billion dollar, multinational corporations, predictive analysis is a critical part of our world in the age of ``Big Data".  
In this paper, I will examine two types of predicitive analysis, Generalized Linear Models (GLM) and Neural Nets (NN).  I will offer a cursory 
overview of the mathematics behind each type of model, and using the R statistical software, I will analyze the accuracy of both GLM and NN 
predictive models, as well as the time needed to construct each model. I will consider an approximation of the standard error as a measure of model 
accuracy, and I will compute these approximations using 50-fold cross-validation.  The models will be fit to two different datasets -- one with a 
normally distributed response variable, and one with a binomially distributed response.  The results -- estimated standard errors and processing 
time -- will be analyzed across the different model types, the different response variables, and different sizes of training sets.
	\end{abstract}
	\vfill\eject


%------------------------------
%-------------body
%------------------------------


%---------------SECTION #1: INTRO, citation of GLM/NN in scholarly works, overview of my study------------------

	\section{Intro to Predictive Analysis.}
		We live in the age of `Big Data'.  Using that data to measure, observe, and predict has great value:  economically, culturally, 
socially.  It's easy to say that this prediction is done by an app -- highlight data, point, click, ta-da!  You can find \href{https://www.google.com/maps/dir/Chicago,+Illinois/Champagne+Beach,+Vanuatu/@10.9584518,-169.5679877,3z/data=!4m13!4m12!1m5!1m1!1s0x880e2c3cd0f4cbed:0xafe0a6ad09c0c000!2m2!1d-87.6297982!2d41.8781136!1m5!1m1!1s0x6ef24c13ce5a7303:0x5807cbc94589a4bb!2m2!1d167.1203814!2d-15.1437919}{estimated travel time}, 
\href{https://www.amazon.com/Statistics-Dummies-Math-Science/dp/1119293529/ref=sr_1_1?ie=UTF8&qid=1518456852&sr=8-1&keywords=statistics+for+dummies+2017}{`Related materials' in consumer recommendations}, and \href{http://www.espn.com/mens-college-basketball/game?gameId=400986579}{predicted outcome of sporting events} with a few simple taps on your 
phone.  But what's the mathematical machinery behind these predictions?  Are all the predictions calculated in a similar manner?  How accurate is a
prediction?  How long would it take to compute a prediction, for a given set of data?  All of these questions are critical to ensure that predictive models 
remain practical and functional in our `Big Data' world.

Now, I will not discuss the specific predictive methods for proprietary applications, like the aforementioned travel predictions from Google or 
suggested product recommendations from Amazon.  But I will offer a cursory examination of two major mathematical methods for predictive analysis:
Generalized Linear Models (GLM), and Neural Networks (NN).  Both methods allow us to `train' mathematical models on an existing data set, which can
subsequently be used to predict future outcomes, or \textbf{response variables}.  Below are two graphs that illustrate, graphically, these two
types of mathematical models for a simple data set.  [[[INSERT GRAPHS OF GLM and NN model of height vs weight data]]]
%Given a sample of heights and weights from 20000 individuals in the United States, the red
%line (in the left image) and the red dots (in the right image) indicate the predicted response variables from mathematical models trained on this
%data.  

%%%%%%%%IMAGE!

These are simple examples; the data set has only two variables, and intuitively most people have an understanding of how these variables relate 
without appealing to mathematics.  But the underlying principle -- that the data can `train' a predictive mathematical model -- is one that we will
observe and analze in many other contexts over the course of this paper.

%---------------SUBSECTION #1.1: GLM in scholarly works---------------------

	\subsection{GLM in Mathematical Literature.}
		For many scientific breakthroughs, there is no one, clear, defining `AHA!' moment where a new idea was discovered from scratch. For
GLM, however, the is a moment that approximates that: the publication in 1972, by Nelder and Wedderburn, of the paper `Generalized Linear Models'
in the \textit{Journal of the Royal Statistical Society}.  This often-cited work clearly laid out many of the ideas that are central to the study 
of GLMs today.  On the back of this work, and with the rise of widely-available statistical software to calculate GLM models such as R and SPSS, 
these models have been widely applied across the scientific literature in service of predictive analysis.  For instance, GLM has utility in
analyzing systems as varied as 
\href{https://www.sciencedirect.com/science/article/pii/S037877961630222X}{hydropower generation}, 
\href{https://www.sciencedirect.com/science/article/pii/S1470160X17306271}{habitat selection for small mammals}, and 
\href{https://www.sciencedirect.com/science/article/pii/S0167668715303358}{insurance claims}.  



Certainly, if GLM is used to obtain predictions of economically or ecologically important data, as suggested in the links above, we would want to 
ensure that the predictions are accurate. We will demonstrate in this paper how to estimate the standard error for a GLM model, which can be 
used to draw up confidence intervals and quantify the accuracy of a prediction.  Having these estimates for accuracy can be vital in determining
whether or not a predictive model is a valid basis for making policy decisions that carry great economic or environmental risk.

%---------------SUBSECTION #1.2: NN in scholarly works----------------------

	\subsection{NN in Mathematical Literature.}
		From the earliest work on \textit{threshold logic} by McCulloch and Pitts in 1943, to the explosion of machine learning algorithms
in use today in the age of `Big Data', the idea that the architecture of computing systems could be inspired by the architecture of the human brain
and its neural networks has maintained a presence in the scientific literature.  With the dramatic rise in computational power in the last 
generation along with statistical software such as R and SPSS, interest in neural networks has flourished, along with its place in scientific
research.  We can see examples of current research using neural nets to analyze topics as diverse as 
\href{https://www.sciencedirect.com/science/article/pii/S1877050918300656}{the gender of Russian authors},
\href{https://www.sciencedirect.com/science/article/pii/S2468203916300024}{the surface radiation in the Sundarban forest in India}, and 
\href{https://www.sciencedirect.com/science/article/pii/S1877050917303617}{routes through the city of Zhengzhou chosen by tourists}.

		
A distinct characteristic of neural nets is that of \textit{machine learning} -- given more data to `train' the model, it can improve the accuracy
of the response variable.  However, adding more data to a neural network requires more computational muscle, and for large or complex datasets,
it can require significantly more time to process.  So, similar to GLM, it will help us to have a means to check how accurate our predictions are,
so we can determine whether or not the added data and accuracy is worth the extra time needed for processing.  As with GLM, our means of assessing
accuracy in this paper will be to approximate the standard error for a given NN model.

%---------------SUBSECTION #1.3: Predictive models and this paper-----------

	\subsection{Methodolgy.}
		For this paper, I will train both GLM and NN models using two pre-selected datasets.  As mentioned above, for each type of
predictive model, we can approximate the standard error for each model, quantifying for us the accuracy of each model.  We can then compare the 
standard errors between the two models, hopefully drawing some conclusions about the relative accuracy of one model compared to the other.

The datasets that will be considered are a listing of various attributes for nearly 54 000 diamonds, and team statistics for nearly 3000 
basketball games from the NBA.  The diamonds data is provided without source as part of the `ggplot2' package for the R statistical software, while
the gamelog dataset was retrieved from the website \href{https://www.basketball-reference.com}{Basketball-Reference.com}.  The diamonds dataset includes 
10 variables for each of the diamonds, while the gamelog dataset looks at 21 statistics for each basketball game.  When creating predictive models, we will 
consider the quantitative variable \textit{price} as our response variable for the diamonds dataset, and the categorical variable \textit{WL} (whether a team won 
or lost a basketball game) as our response variable for the gamelog dataset.  A motivation for working with these two datasets is the fact that the  
response variables for each dataset are distributed differently, as illustrated in the graphs below:

\begin{figure}[h!]
\includegraphics[height=1.5in]{{graph1.png}}
\includegraphics[height=1.5in]{{graph2.png}}
\caption{Distributions for the response variables in DIAMONDS dataset (left), and the GAMELOG dataset (right)}
\end{figure}

We can easily train calculate a GLM or NN model using an entire dataset, and then compare the predicted results from these models against the
actual response variables.  But the weakness of this method is that we are not introducing any new data for testing accuracy.  Instead, dividing 
the data into disjoint `train' and `test' subsets gives us a better idea of how accurate a predictive model would be given new, independent data.
Carrying out this `train-test' process once is prone to great variability, depending on the particular choice of dataset selected to train the 
model.  Instead, repeating this `train-test' process a number of times across repeated random samples of the dataset will give a better
picture of the accuracy for a predictive model.  This repeated resampling and model testing is known as \textit{cross validation}, and we will 
carry it out 50 times (50-fold cross validation) for both GLM and NN predictive models with both of our datasets.

In addition, the size of the `train' subset would likely affect the accuracy of a given model.  With a larger training set, we would anticipate
that a predictive model would be more accurate.  However, as alluded to above, the larger training set also will cost us in terms of processing
time.  To examine how the size of the training set affects model accuracy (and processing time), we will run each instance of 50-fold cross
validation three times, with training sets of size 500, 1000, and 2000.  

To measure accuracy for each of the models, we will calculate the observed standard error of the test subset.  The computations will be carried out
using the open-source R statistical software.




%--------------for now!!
\end{document}



	%---subsection: GLM in scholarly works
	



\section{Generalized Linear Models.}

\subsection{An Introduction to GLM.}
A very basic form of predictive analysis, palatable beyond self-identified `math people', can be found in the form
of linear regression.  Given two quantitative variables, we can approximate a linear relationship between the variables using a basic linear
regression.  This regression line and the data points are easily visualized in a 2-dimensional plot, offering a strong intuitive foundation for the
concept of predictive analysis.  In fact, this method of predicti

\subsection{A basic GLM example.}
One appliction of GLM would be....

The sheer volume of calculation required to construct Generalized Linear Models or Neural Nets for all but the simplest data sets is at once 
unworkable without the aid of statistics software.  The R programming language has packages which bear the burden of these calculations; 

\section{Neural Nets.}

\subsection{An Introduction to Neural Nets.}
To offer a succinct definition of a neural net and how it works would be as simple as explaining how you know a chair when you see 
one.  That is, your experience with all different forms of chairs, all the different styles of chairs, and all the different ways someone might use
a chair allows you to (fairly definitely) state if something is a chair or not.  To use language more similar to those referencing mathematical
models, your lifelong history experiencing chairs is a dataset that has `trained' you to know whether or not a given object is a chair.  In a 
similar vein, if a neural net is `trained' on a given dataset, it can output information about a pre-determined response variable.



\subsection{A basic NN example.}
We can see Neural Nets in action if we consider the data set....  As before, we will use R to construct a Neural Net, from the functions available
in the -- surprise! -- `neuralnet' package.

\section{A Comparison of GLM and NN.}
Having looked at both GLM and Neural Nets individually, now we can examine more closely how the two methods stack up when they aim to create a 
predictive model for the same data set.  Which model offers more accurate predictions?  What data set offers more consistent predictions?  Is there
an appreciable difference between the two methods?  To weigh in on this comparison, I will (1) demonstrate how to divide a data set into `train' 
and `test' subsets so we can create and test predictive models (2) outline R code that calculates predictive models for both methods, (3) compare 
the responses from the two predictive models, both numerically and graphically, and (4) draw some observations from the comparisons.

\subsection{A Data Set for our Predictions.}
In the `real world' of big data predictions, there is collected data and future data.  The collected data is used to create a predictive model, and
the hope is it can be tested (and refined) in the future as more, newer data becomes available.  Considering present/future data is not feasible 
for this paper; I'm not going to create a predictive model here with current and come back to write Volume \#2 in six months after testing future 
data.  We need to `simulate' current and future data from existing, collected data.

To accomplish this, the R package `caTools' offers the function `sample.split'.  This function allows us to ....

\subsection{Creating Predictive Models in R.}
With our `train' and `test' data sets in hand, it's time to create predictive models.  In previous sections, we saw how to accomplish this in R.  
However, since our focus is on offering a general comparison for ANY multi-variable data set, we need to fine-tune the code a bit.  To that end...

\subsection{A Comparison of the Predictive Models.}
The above code allowed us to see, side-by-side, the predicted and actual responses for our data set, and the standard errors for the predictions.
We can analyze these outputs both numerically and graphically.  For instance....

\subsection{Observations from the Comparison.}
After looking at the predictions and responses from the two methods, we can see that....


\section{Topics for Further Study.}
Having completed a very basic side-by-side comparison of GLM and Neural Net predictive methods for data sets, we can begin to focus on some 
questions that might offer opportunities for further exploration.  For instance....



%%%bibliography, etc

\end{document}


  A profoundly basic example of such a mathematical model involves a data set that measures the 
amount of lemons imported to the United States from Mexico annually and the U.S. highway fatality rate.  As seen in the graph below, the line 
represents a mathematical model (created by linear regression, a very specific case of GLM) that allows us to predict future outcomes.  

%%include graphic!!

From this mathematical model, created by a method closely related to GLM, we can predict future values of the highway fatality rate if we know how 
many lemons are brought into the US from Mexico for a given year.  (If you can't detect my sarcasm in the previous sentence, please see a statistics 
professor right away!  And be very careful if you're buying lemons and driving home!)  The prediction from this model -- the highway fatality rate
-- is called the \textbf{response variable}, and all predictions from a GLM and NN model takes the form of a response variable.  Another example to 
illustrate this point uses a data set relating the height and weight of 20000 individuals in the US.  

Again, the line represents a linear regression




The mathematical theory that describes WHY the training process works will not be 
discussed in this paper (but more information can be found in the `Further Reading' section), but I will give a detailed account of HOW the 
training process can be carried out, using statistical software.  

, if we 
specify the outcome or response variable of interest to us.  That is, we identify what variable we are interested in predicting, 

create predictive models from a set of collected data that allow us to
predict values for a chosen response variable.  The mathematical machinery that allows both methods (GLM and NN) to `work'  will be discussed. 
We will see how these models can handle different types of data sets: quantitative data (for instance, the travel time between two locations) or 
categorical data (whether a team wins or loses a sporting event).  And, looking at the two methods side-by-side, we will use both methods to draw
predictions from the same data set, and look for notable distinctions between the two methods in terms of consistency or accuracy or predictions.

Aside from the broad overview of these predictive methods discussed in this paper, applications for both methods can be seen in ....LITERATURE 
"REVIEW".
