%----------495 Report, Chapter 1----------

%------------------------------------------------------------------------------------------------------------------------------------------------------
%---------------SECTION #1: INTRO, citation of GLM/NN in scholarly works, statistical software in pred. modelling------------------
%------------------------------------------------------------------------------------------------------------------------------------------------------

	\section{Intro to Predictive Analysis.}
		We live in the age of `Big Data'.  Using that data to measure, observe, and predict has great value:  economically, culturally, 
socially.  It's easy to say that this prediction is done by an app -- highlight data, point, click, ta-da!  You can find \href{https://www.google.com/maps/dir/Chicago,+Illinois/Champagne+Beach,+Vanuatu/@10.9584518,-169.5679877,3z/data=!4m13!4m12!1m5!1m1!1s0x880e2c3cd0f4cbed:0xafe0a6ad09c0c000!2m2!1d-87.6297982!2d41.8781136!1m5!1m1!1s0x6ef24c13ce5a7303:0x5807cbc94589a4bb!2m2!1d167.1203814!2d-15.1437919}{estimated travel time}, 
\href{https://www.amazon.com/Statistics-Dummies-Math-Science/dp/1119293529/ref=sr_1_1?ie=UTF8&qid=1518456852&sr=8-1&keywords=statistics+for+dummies+2017}{`Related materials' in consumer recommendations}, and \href{http://www.espn.com/mens-college-basketball/game?gameId=400986579}{predicted outcome of sporting events} with a few simple taps on your 
phone.  But what's the mathematical machinery behind these predictions?  Are all the predictions calculated in a similar manner?  How accurate is a
prediction?  How long would it take to compute a prediction, for a given set of data?  All of these questions are critical to ensure that predictive models 
remain practical and functional in our `Big Data' world.

Now, I will not discuss the specific predictive methods for proprietary applications, like the aforementioned travel predictions from Google or 
suggested product recommendations from Amazon.  But I will offer a cursory examination of two major mathematical methods for predictive analysis:
Generalized Linear Models (GLM), and Neural Networks (NN).  Both methods allow us to `train' mathematical models on an \textit{observed data set}, which can subsequently be used to predict future outcomes, or \textbf{response variables}.  

Figure \#\ref{intromodels} contains two graphs that illustrate, graphically, these two types of mathematical models for a simple data set.
Given a sample of heights and weights from 20000 individuals in the United States collected from a 2015 survey conducted by the Centers for Disease
Control \cite{brfss}, the red lines indicate the predicted response variables from mathematical models trained on this data.  

%----------No chunk, just used 'chunk11.R' file to create graphs
	
%	%load cdc data and create models and graphs.
%	<<chunk11,echo=F,results='hide',fig.keep='all'>>=
%		invisible(source("R/chunk11.R"))
%	@

%------------------------------------------------
		
		%intromodels
		\begin{figure}[h!]
		\centering
		\includegraphics[width=2.8in]{{model1.png}}
		\includegraphics[width=2.8in]{{model2.png}}
		\caption{Predictive models for height and weight data, trained using GLM methods (left), and NN methods (right)}
		\label{intromodels}
		\end{figure}
		%intromodels

These are simple examples; the data set has only two variables, and intuitively most people have an understanding of how these variables relate 
without appealing to mathematics.  But the underlying principle -- that the data can `train' a predictive mathematical model -- is one that we will
observe and analyze in many other contexts over the course of this paper.

%---------------SUBSECTION #1.1: GLM in scholarly works---------------------

	\subsection{GLM in Mathematical Literature.}
		For many scientific breakthroughs, there is no one, clear, defining `AHA!' moment where a new idea was discovered from scratch. For
GLM, however, the is a moment that approximates that: the publication in 1972, by Nelder and Wedderburn, of the paper `Generalized Linear Models'
in the \textit{Journal of the Royal Statistical Society} \cite{agresti}.  This often-cited work clearly laid out many of the ideas that are central to the study 
of GLMs today.  On the back of this work, and with the rise of widely-available statistical software to calculate GLM models such as R and SPSS, 
these models have been widely applied across the scientific literature in service of predictive analysis.  For instance, GLM has utility in
analyzing systems as varied as 
\href{https://www.sciencedirect.com/science/article/pii/S037877961630222X}{hydropower generation}, 
\href{https://www.sciencedirect.com/science/article/pii/S1470160X17306271}{habitat selection for small mammals}, and 
\href{https://www.sciencedirect.com/science/article/pii/S0167668715303358}{insurance claims} \cite{scidirect}.  

Certainly, if GLM is used to obtain predictions of economically or ecologically important data, as suggested in the links above, we would want to 
ensure that the predictions are accurate. We will demonstrate in this paper how to estimate the standard error for a GLM model, which can be 
used to draw up confidence intervals and quantify the accuracy of a prediction.  Having these estimates for accuracy can be vital in determining
whether or not a predictive model is a valid basis for making policy decisions that carry great economic or environmental risk.

%---------------SUBSECTION #1.2: NN in scholarly works----------------------

	\subsection{NN in Mathematical Literature.}
		From the earliest work on \textit{threshold logic} by McCulloch and Pitts in 1943, to Frank Rosenblatt's Perceptron,
to the explosion of machine learning algorithms
in use today in the age of `Big Data', the idea that the architecture of computing systems could be inspired by the architecture of the human brain
and its neural networks has maintained a presence in the scientific literature \cite{kurenkov}.  
With the dramatic rise in computational power in the last 
generation along with statistical software such as R and SPSS, interest in neural networks has flourished, along with its place in scientific
research.  We can see examples of current research using neural nets to analyze topics as diverse as 
\href{https://www.sciencedirect.com/science/article/pii/S1877050918300656}{the gender of Russian authors},
\href{https://www.sciencedirect.com/science/article/pii/S2468203916300024}{the surface radiation in the Sundarban forest in India}, and 
\href{https://www.sciencedirect.com/science/article/pii/S1877050917303617}{routes through the city of Zhengzhou chosen by tourists} \cite{scidirect}.
		
A distinct characteristic of neural nets is that of \textit{machine learning} -- given more data to `train' the model, it can improve the accuracy
of the response variable.  However, adding more data to a neural network requires more computational muscle, and for large or complex datasets,
it can require significantly more time to process.  So, similar to GLM, it will help us to have a means to check how accurate our predictions are,
so we can determine whether or not the added data and accuracy is worth the extra time needed for processing.

%---------------SUBSECTION #1.3: Accuracy of predictive models-----------

	\subsection{Use of Statistical Software in Predictive Models.}
		In dealing with `Big Data' and attempting to address complex, computationally-intensive problems (such as building a predictive 
model), the approaches have hinged on the technology available.  The twentieth century began with 
\href{https://magazine.amstat.org/blog/2006/09/01/origins-of-statistical-computing/}{punch card tabulating} \cite{grier} but by the latter half 
of the century, statistical analysis had moved largely to computers.  Momentum for 
\href{https://www.stat-computing.org/computing/history/chambers-ryan.html}{the trend of statistical software grew during the 1960s} \cite{chambers},
and the release of SPSS in 1968 was a critical moment in making statistical analysis more widely available beyond actuarials and computer 
scientists.  Similiarly, the SAS software and S programming language, released in 1976, helped to make statistical analysis accessible to a wide 
range of scientists.  Today, SPSS, SAS, and R (the modernized version of S) or similar software built in their mold are indispensable across the 
sciences and humanities for statistical analysis \cite{ozgur}.  A small sampling of how these statistical languages are used in social science research:
\href{https://www.sciencedirect.com/science/article/pii/S0277953617305105}{SPSS is used to analyze why happiness matters}, 
\href{https://www.sciencedirect.com/science/article/pii/S0277953616301095}{SAS is used to consider the intersection of bisexuality, poverty, and mental health} \cite{scidirect},
\href{https://www.kellogg.northwestern.edu/news-events/conference/ic2s2/2018/workshops-and-datathon.aspx}{and whole conferences are devoted to finding uses for R in the social sciences}.

For this paper, all analysis was performed using R.  The preference for R, as opposed to SAS or SPSS, owes to the fact that it supports both the 
creation of both GLM and NN predictive models, its wide acceptance across industry (note the conference above!), and it is freely available 
through the GNU General Public License.  
\vfill\eject

Throughout this paper, excerpts of R code will appear as blocktext as seen below:

	<<chunk12, echo=2, comment=NA, background="grey80">>=
	greeting<-"Hello World!!"
	print(greeting)
	@

When snippets of R code appear within paragraphs, they will be formatted in the \texttt{Typewriter} font.

%---------------SUBSECTION #1.4: Overview of this Analysis-----------

	\subsection{An Overview of Our Analysis.}
	The focus of this paper is to compare the accuracy of both GLM and NN predictive models, given that we use \textit{accuracy} to refer to the
standard error for a model's predictions.  The accuracy of the predictive models is examined across a number of comparisons:  we will examine the 
accuracy between GLM and NN models, the accuracy for predictive models with a qualitative response versus predictive models with a categorical 
response, and the accuracy for predictive models built on datasets of varying sizes.
  
	Before the models are built, tested, and analyzed, in chapter \#2 a broad introduction to some of the methodology and terminology for 
predictive modeling is given. An overview of the components essential to all predictive models is given in \S2.1, and aspects specific to GLM and 
NN modelling are introducted in Sections \S2.2 and \S2.3, respectively.  

	In chapter \#3 the models are built, tested, and analyzed.  In section \S3.1 the datasets for analysis -- one with a quantitative response, 
one with a categorical response -- are introduced.  In \S3.2 the R function used to build and test the models for analysis will be introduced, along
with the specific function calls that allow us to obtain data for analysis.  In \S3.3 we summarize the results, and offer cursory analysis of the 
findings.



